---
title: "word2vec_pairwise"
author: "Ben Morris"
date: "11/2/2017"
output: html_document
---


```{r, echo=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(devtools))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(wordVectors))
suppressPackageStartupMessages(library(wordbankr))
suppressPackageStartupMessages(library(shiny))
suppressPackageStartupMessages(library(shinyBS))
suppressPackageStartupMessages(library(visNetwork))
suppressPackageStartupMessages(library(tsne))
suppressPackageStartupMessages(library(networkD3))
suppressPackageStartupMessages(library(igraph))
suppressPackageStartupMessages(library(lazyeval))
suppressPackageStartupMessages(library(langcog))
suppressPackageStartupMessages(library(entropy))



english_cats <- get_item_data("English (American)", "WS") %>%
  filter(type=="word") %>%
  distinct(uni_lemma, .keep_all=TRUE) %>%
  select(uni_lemma, category)


english_items <- get_item_data("English (American)", "WS") %>%
  filter(type =='word') %>%
  select(uni_lemma) %>%
  distinct() 

head(english_items)
#NOTE: This drops "an" from the cdi. Might be other cases where two definitions map onto the same uni_lemma


setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

```{r}
model <- read.vectors("w2v_vectors/CHILDES_stemmed_vectors.bin")

######## Special case code needs to be updated to deal with stemming/lemmatization ####### 

# Read back the csv with a new colum of replacement lemmas
missing_items <- read_csv("data/missing_lemmas.csv")

all_cdi_words <- unique(c(as.vector(as.matrix(missing_items)),
                        english_items$uni_lemma))
all_cdi_stemmed <- read_csv('data/cdi_words_stemmed.txt', col_names='uni_lemma')
#11.2.2017, spaghetti no longer a thing in cdi?
all_cdi_stemmed <- all_cdi_stemmed$uni_lemma[-which(all_cdi_stemmed$uni_lemma == "spaghetti")]

cdi_stemmed_and_unstemmed <- data.frame(all_cdi_words, all_cdi_stemmed)

#filter the matrix to only include CDI words
model_wordbank <- model[[which(rownames(model) %in% all_cdi_stemmed), average=FALSE]]
model_wordbank_sorted <-model_wordbank[sort(rownames(model_wordbank)),]

#Special cases
special_cases <- missing_items %>%
  filter(!is.na(addword) | !is.na(subword))

rename_cases <- missing_items %>%
  filter(is.na(addword) & is.na(subword),
         !is.na(word)) %>%
  arrange(uni_lemma)

extra_rows <- rownames(model_wordbank_sorted)  %in% rename_cases$word
rownames(model_wordbank_sorted)[extra_rows] <- rename_cases$uni_lemma

special_case_vector <- function(special_case) {
  if(!is.na(special_case$addword)) {
   model_wordbank[special_case$word,] + 
      model_wordbank[special_case$addword,]
  } else {
    model_wordbank[special_case$word,] - 
      model_wordbank[special_case$subword,]
  }
}

special_case_vectors <- t(sapply(1:nrow(special_cases), 
                               function(x) special_case_vector(special_cases[x,]), 
                               simplify = "matrix"))
rownames(special_case_vectors) <- special_cases$uni_lemma

model_wordbank_specialcases <- rbind(model_wordbank_sorted, special_case_vectors)

model_wordbank_unilemmas <- as.VectorSpaceModel(model_wordbank_specialcases[rownames(model_wordbank_specialcases)
                                                        %in% english_items$uni_lemma, ])

model_wordbank_unilemmas <- as.VectorSpaceModel(model_wordbank_sorted[rownames(model_wordbank_sorted)
                                                        %in% english_items$uni_lemma, ])

#create word similarity matrix
wordbank_sim <- cosineSimilarity(model_wordbank_unilemmas, model_wordbank_unilemmas)

#set to zero all the negative values in the matrix
wordbank_zeroed <- ifelse(wordbank_sim < 0, 0, wordbank_sim)
```





```{r}
wordbank_fixed <- wordbank_zeroed

#normalizing to 1
# wordbank_normalized <- prop.table(wordbank_zeroed, margin = 1)


# test <- wordbank_zeroed[1:6,1:6]
wordbank_fixed[lower.tri(wordbank_fixed, diag=TRUE)] <- NA
wordbank_fixed<-cbind(thing1=rownames(wordbank_fixed),wordbank_fixed)

pairwise_sim <- gather(as.data.frame(wordbank_fixed), key=thing2, value=sim, -thing1)
# english_cats <- english_ws_items %>% select(uni_lemma, category) %>% as.data.frame(.)

pairwise_sim <- pairwise_sim %>% filter(!is.na(sim))
#coercing new rows on accident?
pairwise_cats <- pairwise_sim %>% 
  inner_join(english_cats, by=c("thing1"="uni_lemma")) %>%
  rename(category1=category) %>% 
  left_join( english_cats, by=c("thing2"="uni_lemma")) %>%
  rename(category2=category)



pairwise_cats <- pairwise_cats %>% 
  mutate(sameCat = ifelse(category1==category2, 1, 0)) %>%
  mutate(sim=as.numeric(sim))

ggplot(data=pairwise_cats, aes(x=sim)) +
         geom_histogram(aes(x=sim)) + xlab("W2V Pairwise Cosine Similarity")
```


#p-p-p-PLOTS!
```{r}
#plot of word2vec pairwise sim within categories versus across (*DIFFCATS)
diff_cats_colors <- scales::hue_pal(h = c(0, 360))(23)
diff_cats_colors[1] <- "#000000"

pairwise_cats %>% 
  mutate(category1=ifelse(sameCat==0, "*DIFFCATS", category1)) %>%
  group_by(category1, sameCat) %>%
  summarize(avg_sim=mean(sim, na.rm=TRUE), sd=sd(sim)) %>%
  ggplot(aes(x=category1, y=avg_sim, group=category1)) +
  geom_bar(stat="identity",position="dodge", aes(x=as.factor(category1), y=avg_sim, fill=category1)) +
  geom_linerange(stat="identity", position="dodge", aes(ymax=avg_sim+sd, ymin=avg_sim-sd)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
  labs(x="Category, if Same", y="Mean Pairwise Similarity from W2V") +
  scale_fill_manual(values=diff_cats_colors)


#each cat, avg vs diff categories, or same cat
pairwise_cats %>% 
  group_by(category1, sameCat) %>%
  summarize(avg_sim=mean(sim, na.rm=TRUE), sd=sd(sim)) %>%
  ggplot(aes(x=category1, y=avg_sim, group=sameCat)) +
  geom_bar(stat="identity",position="dodge", aes(x=as.factor(category1), y=avg_sim, fill=as.factor(sameCat))) +
  geom_linerange(stat="identity", position=position_dodge(.9), aes(ymax=avg_sim+sd, ymin=avg_sim-sd)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
  labs(x="Target Word Category", y="Mean Pairwise Similarity from W2V") +
  scale_fill_manual(values=c('black', 'steelblue1'), name="Paired Word Category", 
                    labels=c('Different (All Averaged)', 'Same Category'))
  

#full spread, every possible category pair
pairwise_cats %>% 
  group_by(category1, category2, sameCat) %>%
  summarize(avg_sim=mean(sim, na.rm=TRUE), sd=sd(sim)) %>%
  ggplot(aes(x=category1, y=avg_sim, group=category2)) +
  geom_bar(stat="identity",position="dodge", aes(x=as.factor(category1), y=avg_sim, fill=category2, alpha=as.factor(sameCat))) +
  # geom_linerange(stat="identity", position="dodge", aes(ymax=avg_sim+sd, ymin=avg_sim-sd, color=category2)) +
  theme_bw() +
  # facet_grid(~category2) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
  # coord_cartesian(ylim=c(0,.015)) +
  labs(x="Target Word Category", y="Mean Pairwise Similarity from W2V") +
  scale_fill_hue(name="Paired Word Category") +
  scale_alpha_manual(values=c(.4,1), name="", labels=c('Different Categories', 'Same Categories'))


```

```{r}
plot_me_sim <- function(data, groupingVar, yVar) {
  groupingVar <- enquo(groupingVar)
  yVar<- enquo(yVar)
  # optionalGroup2 <- enquo(optionalGroup2)
  new_data <- data %>%
    group_by(!!groupingVar) %>%
    summarize(avg_sim=mean(sim, na.rm=TRUE), sd=sd(sim))
  new_plot <- ggplot(new_data, aes_q(x=groupingVar, y=yVar, group=groupingVar)) +
    geom_bar(stat="identity",position="dodge", aes(x=as.factor(groupingVar), y=yVar, fill=groupingVar, color=groupingVar)) 
  print(new_plot)
}

pairwise_cats %>% plot_me_sim(category1, avg_sim)

```



```{r}
library(feather)
tmp <- read_feather("~/Downloads/coded_responses.feather")
head(tmp)

tmp_lagged <- tmp %>%
  group_by(subj,age) %>%
  mutate(lag_referent = lag(referent))

# losing 600 of the referents?
model_ldp <- model[[which(rownames(model) %in% tmp_lagged$referent), average=FALSE]]
ldp_referents <- tmp %>% distinct(referent)
problem_cases <- ldp_referents[which(!ldp_referents$referent %in% rownames(model)),]

find_missing <- data.frame(ref= rownames(model)) %>%
  mutate(found_one = ifelse(grepl(ref,"ho"), 1,0))

matches <- unique (grep(paste(rownames(model),collapse="|"), 
                        problem_cases, value=TRUE))


# create a similarity matrix
ldp_sim <- cosineSimilarity(model_ldp, model_ldp)
#set to zero all the negative values in the matrix
ldp_zeroed <- ifelse(ldp_sim < 0, 0, ldp_sim)

ldp_fixed <- ldp_zeroed
ldp_fixed[lower.tri(ldp_fixed, diag=TRUE)] <- NA
ldp_fixed<-cbind(thing1=rownames(ldp_fixed), ldp_fixed)

ldp_sim <- gather(as.data.frame(ldp_fixed), key=thing2, value=sim, -thing1)

ldp_sim <- ldp_sim %>% filter(!is.na(sim)) %>% mutate(sim=as.numeric(sim))


ggplot(data=ldp_sim, aes(x=sim)) +
         geom_histogram(aes(x=sim)) + xlab("W2V Pairwise Cosine Similarity")

hm <- tmp_lagged %>% left_join(ldp_sim, by=c('referent'='thing1', 'lag_referent' = 'thing2'))
testing <- hm %>% 
  filter(!is.na(sim)) %>%
  #these one-to-one cases are already excluded because we NA'd the diagnal of the similarity matrix
  filter(!lag_referent == referent) %>%
  mutate(sim=as.numeric(sim))

ggplot(data=testing, aes(x=sim)) +
         geom_histogram(aes(x=sim)) + labs(x="W2V Cosine Sim of Current Referent and Lag Referent", y="Frequency")
```


```{r}
# ex
examp1 <- "When discussing performance with colleagues, teaching, sending a bug report or searching for guidance on mailing lists and here on SO, a reproducible example is often asked and always helpful. What are your tips for creating an excellent example? "
examp2 <- "Sometimes the problem really isn't reproducible with a smaller piece of data, no matter how hard you try, and doesn't happen with synthetic data (although it's useful to show how you produced synthetic data sets that did not reproduce the problem, because it rules out some hypotheses). " 
examp3 <- "You are most likely to get good help with your R problem if you provide a reproducible example. A reproducible example allows someone else to recreate your problem by just copying and pasting R code. There are four things you need to include to make your example reproducible: required packages, data, code, and a description of your R environment."
examp4 <- "Do your homework before posting: If it is clear that you have done basic background research, you are far more likely to get an informative response. See also Further Resources further down this page. Do help.search(keyword) and apropos(keyword) with different keywords (type this at the R prompt)."
examp5 <- "Before asking a technical question by e-mail, or in a newsgroup, or on a website chat board, do the following:  Try to find an answer by searching the archives of the forum you plan to post to. Try to find an answer by searching the Web. Try to find an answer by reading the manual. Try to find an answer by reading a FAQ."


df <- data.frame(ID = sapply(1:5, function(i) paste0(sample(letters, 5), collapse = "")),
                 txt = sapply(1:5, function(i) eval(parse(text=paste0("examp",i))))
                 )

m <- list(ID = "ID", Content = "txt")
myReader <- readTabular(mapping = m)
mycorpus <- Corpus(DataframeSource(df), readerControl = list(reader = myReader))

# Manually keep ID information from https://stackoverflow.com/a/14852502/1036500
for (i in 1:length(mycorpus)) {
  attr(mycorpus[[i]], "ID") <- df$ID[i]
}

```



```{r}
library(tm)
library(topicmodels)

# m <- list(ID=)
m <- list(ID = "subj", Content = "chat")
myReader <- readTabular(mapping = m)
tmp_fixed <- tmp %>% 
  select(subj,chat) %>%
  as.data.frame()

mycorpus <- Corpus(DataframeSource(tmp_fixed), readerControl = list(reader = myReader))



tmp_fixed <- tmp %>% filter(!is.na(referent)) %>% ungroup() %>% group_by(subj, age) %>%
  summarize(referents=paste(referent, collapse=' ')) %>% 
  as.data.frame()

tmp_window <- tmp %>% filter(!is.na(referent)) %>% ungroup() %>%
  mutate(run=rep(1:(nrow(.)/10), each=10)) %>% 
  group_by(run) %>%
  summarize(referents=paste(referent, collapse=' '))
# # ------------------------------


hm <- VectorSource(unlist(lapply(tmp_window$referents, as.character)))
corpus <- SimpleCorpus(hm)

docs <-tm_map(corpus,content_transformer(tolower))
toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, toSpace, "’")
docs <- tm_map(docs, toSpace, "‘")
docs <- tm_map(docs, toSpace, ".")
docs <- tm_map(docs, toSpace, "?")

docs <- tm_map(docs, removePunctuation)

# 
writeLines(as.character(corpus[[1]]))

# 



#Create document-term matrix
dtm <- DocumentTermMatrix(corpus)
#convert rownames to filenames
filenames <- tmp_fixed %>%  mutate(id = paste(subj, age, sep="_")) %>% select(id) %>% as.vector(.)
rownames(dtm) <- filenames$id
#collapse matrix by summing over columns
freq <- colSums(as.matrix(dtm))
#length should be total number of terms
length(freq)
#create sort order (descending)
ord <- order(freq,decreasing=TRUE)
#List all terms in decreasing order of freq and write to disk
freq[ord]
# write.csv(freq[ord],"word_freq.csv")

```



```{r}
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
 

#Number of topics
k <- 5


#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, 
                                                 burnin = burnin, iter = iter, thin=thin))
 

#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
# write.csv(ldaOut.topics,file=paste("LDAGibbs",k,"DocsToTopics.csv"))

ldaOut.terms <- as.matrix(terms(ldaOut,6))



topicProbabilities <- as.data.frame(ldaOut@gamma)

#Find relative importance of top 2 topics
topic1ToTopic2 <- lapply(1:nrow(dtm),function(x)
sort(topicProbabilities[x,])[k]/sort(topicProbabilities[x,])[k-1])
 
```

