---
title: "word2vec_morris"
output: html_document
---


```{r, echo=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(devtools))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(wordVectors))
suppressPackageStartupMessages(library(wordbankr))
suppressPackageStartupMessages(library(readr))
suppressPackageStartupMessages(library(shiny))
suppressPackageStartupMessages(library(shinyBS))
suppressPackageStartupMessages(library(visNetwork))
suppressPackageStartupMessages(library(tsne))
suppressPackageStartupMessages(library(networkD3))
suppressPackageStartupMessages(library(igraph))
suppressPackageStartupMessages(library(lazyeval))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(langcog))
suppressPackageStartupMessages(library(entropy))


#wordbank
# https://github.com/langcog/wordbank
# eng_data <- get_instrument_data("English", "WS")
# 
# item_info <- read_csv("~/downloads/[English_WS].csv") %>%
#  mutate(num_item_id = as.numeric(sub("item_", "", itemID))) %>%
#  select(type, category, uni_lemma, num_item_id)
# 
# eng_iteminfo_data <- left_join(eng_data, item_info)



english_items <- get_item_data('English', 'WS') %>%
  filter(type =='word') %>%
  select(uni_lemma) %>%
  distinct() 
#NOTE: This drops "an" from the cdi. Might be other cases where two definitions map onto the same uni_lemma


# source: https://github.com/bmschmidt/wordVectors#quick-start
#"There's a lot of other stuff you can do besides just measuring nearness: you can do analogies, projection, and more complicated plots. But for that you should read my blog posts on this"
```



Training word2vec on CHILDES Data
```{r, echo=FALSE, cache= TRUE}
# full childes
# model = train_word2vec("Corpora/CHILDES_words_stemmed.txt",
#                             output="w2v_vectors/CHILDES_stemmed_vectors.bin", threads = 4,
#                             vectors = 100, window=6, cbow=1, min_count = 10, force= TRUE)



model = read.vectors("w2v_vectors/CHILDES_stemmed_vectors.bin")

######## Special case code needs to be updated to deal with stemming/lemmatization ####### 
# Write a csv of the missing lemmas to join back in
# english_items_missing <- english_items %>%
  #filter(!uni_lemma %in% rownames(model)) %>%
  #write_csv("missing_lemmas.csv")

# Read back the csv with a new colum of replacement lemmas
missing_items <- read_csv("data/missing_lemmas.csv")

all_cdi_words <- unique(c(as.vector(as.matrix(missing_items)),
                        english_items$uni_lemma))
all_cdi_stemmed <- read_csv('data/cdi_words_stemmed.txt', col_names='uni_lemma')
cdi_stemmed_and_unstemmed <- data.frame(all_cdi_words, all_cdi_stemmed)

#filter the matrix to only include CDI words
model_wordbank <- model[[which(rownames(model) %in% all_cdi_stemmed$uni_lemma), average=FALSE]]
model_wordbank_sorted <-model_wordbank[sort(rownames(model_wordbank)),]

#Special cases
special_cases <- missing_items %>%
  filter(!is.na(addword) | !is.na(subword))

rename_cases <- missing_items %>%
  filter(is.na(addword) & is.na(subword),
         !is.na(word)) %>%
  arrange(uni_lemma)

extra_rows <- rownames(model_wordbank_sorted)  %in% rename_cases$word
rownames(model_wordbank_sorted)[extra_rows] <- rename_cases$uni_lemma

special_case_vector <- function(special_case) {
  if(!is.na(special_case$addword)) {
   model_wordbank[special_case$word,] + 
      model_wordbank[special_case$addword,]
  } else {
    model_wordbank[special_case$word,] - 
      model_wordbank[special_case$subword,]
  }
}

special_case_vectors <- t(sapply(1:nrow(special_cases), 
                               function(x) special_case_vector(special_cases[x,]), 
                               simplify = "matrix"))
rownames(special_case_vectors) <- special_cases$uni_lemma

model_wordbank_specialcases <- rbind(model_wordbank_sorted, special_case_vectors)

model_wordbank_unilemmas <- as.VectorSpaceModel(model_wordbank_specialcases[rownames(model_wordbank_specialcases)
                                                        %in% english_items$uni_lemma, ])

#create word similarity matrix
wordbank_sim <- cosineSimilarity(model_wordbank_unilemmas, model_wordbank_unilemmas)
wordbank_dist <- cosineDist(model_wordbank, model_wordbank)

#set to zero all the negative values in the matrix
wordbank_zeroed <- ifelse(wordbank_sim < 0, 0, wordbank_sim)
#also set the self weights to zero before normalizing
diag(wordbank_zeroed) <- 0

#"normalize the rows of the matrix, such that each row sums to one, and that any value DM(I,J) can be interpreted as the strength of the directional connection from word WI to word WJ" (Rotaru, 2016)
# normalizing seems inapporpriate for our data. it would make lead to the loss of the symmetry of 
#   our matrix and thus force directionality where there was none. 
# wordbank_normal <- normalize_lengths(wordbank_zeroed)

#trying to  normalize akin to Rotaru...
wordbank_sum1 <- prop.table(wordbank_zeroed, margin = 1)


# Rotaru 2016 also does the following step: "(f) consider the discrete Markov chain associated with DM, which we denote as MARKOV(DM), and compute the state of MARKOV(DM) at steps K = 1 through K = 7, namely SK(DM); (g) for each word W and each K between 1 and 7, count the number of close neighbours of W (numNeighK). A word V is considered a close neighbour of W if P(V |SK(DM)) > threshK, where threshK are lower thresholds."

#example
ex <- matrix(c(0,0.5,0.5,.5,0,.5,.5,.5,0),nrow = 3,
    byrow = TRUE) #define the transition matrix
dtmc_ex <- new("markovchain",transitionMatrix=ex,
    states=c("a","b","c"),
    name="MarkovChain A") #create the DTMC
dtmc_ex
plot(dtmc_ex)
dtmc_ex['b', 'c']
transitionProbability(dtmc_ex, 'a', 'b')
conditionalDistribution(dtmc_ex, 'a')

#simulate steps
initialState<-c(0,1,0)
steps<-1
finalState<-initialState*dtmc_ex^steps #using power operator
finalState




dtmcA<-as(wordbank_sum1, "markovchain") 
conditionalDistribution(dtmcA, 'moo')

#hmmmm how do Rotaru et al generate the lower threshold?
hist(rowSums(dtmcA@transitionMatrix>0.008))

# are steps done for each node?
starting_matrix <- matrix(ncol=nrow(wordbank_zeroed))
step_through_MK <- function(transition_matrix, num_steps, start_with_number_1, states_matrix) {
    if (start_with_number_1==(nrow(wordbank_zeroed)+1)) {
        return(states_matrix)
    } else 
    initial_state <- rep(0,nrow(wordbank_zeroed))
    initial_state[start_with_number_1]=1
    finalState <- initial_state*transition_matrix^num_steps
    final_states <- rbind(states_matrix, finalState)
    step_through_MK(transition_matrix, num_steps, start_with_number_1 + 1, final_states)
}

step1 <- dtmcA@transitionMatrix
step2 <- step_through_MK(dtmcA, 2, 1, starting_matrix)

hist(rowSums(wordbank_dist))


#write the matirx for visualization
#NOTE:if you rewrite the csv file, the network will NOT load when you run the shiny app
# until you manually enter 'in_node' (without quotes) as the value for the first cell of the data sheet.
#write.csv(wordbank_zeroed, file= 'shiny_apps/networks/assocs/w2v_assocs.csv')



###ALLLLL WORDS###
# full_corpus_sim <- cosineSimilarity(model,model)
# full_corpus_zeroed <- ifelse(full_corpus_sim < 0, 0, full_corpus_sim)
# diag(full_corpus_zeroed) <- 0
# 
# full_degree <- rowSums(full_corpus_zeroed) 
# all_all_items <- rownames(full_corpus_zeroed)
# 
# ordered_all_words <- sort(full_degree, decreasing=TRUE)
# 
# cdi_with_all_words <- (rowSums(full_corpus_zeroed[rownames(full_corpus_zeroed) 
#                                                   %in% all_cdi_words,]))
# cdi_with_all_words <- sort(cdi_with_all_words, decreasing = TRUE)
# cdi_with_all_words[1:50]
# 
# pref_acquisiton_ordered <- order(probability, decreasing= TRUE)
# all_items[pref_acquisiton_ordered[1:50]]
```



CLEAN UP THE WORDBANK DATA AND MATCH TO WORDS PRESENT IN OUR MODEL
```{r, echo=FALSE}
english_data_messy <- 
  get_instrument_data("English", "WS", administrations = TRUE, iteminfo=TRUE) %>%
  filter(type == "word", uni_lemma %in% rownames(wordbank_zeroed)) %>%
  select(-production, -comprehension) 

english_data_distinct <- english_data_messy %>%
  ungroup() %>%
  mutate(value = value == "produces") %>%
  group_by(data_id, age, uni_lemma) %>%
  summarize(value = if_else(sum(value) > 0, "produces", ""))


# Production values from wordbank are inappropriate to use since we have subsetted down to 
# the words present in our network. Need to recalculate vocabulary size.
fixed_production <- english_data_distinct %>% 
  group_by(data_id) %>%
  filter(value=='produces') %>%
  summarize(vocab_size=n())

english_data <- left_join(english_data_messy, fixed_production, by='data_id')



english_kids <- 
  get_administration_data(language = "English", form = "WS", original_ids = TRUE)

all_items <- rownames(wordbank_zeroed)
```



**TRANSITIVITY** (Clustering Coefficient), Kid Data and Randomly Generated Graphs
```{r}
###### CODE FOR BUILDING THE RANDOM NETWORKS ######
#function to call for just one kid's data
pull_one <- function(id) {
  one_kid <- (english_data %>%
  filter(data_id == id) %>%
  filter(value == "produces"))
}


# # set a function for making a random graph of size N and computing CC
# sample_transitivity <- function(num_nodes) {
#     random_nodes <- sample(all_items, num_nodes)
# 
#     random_graph <- graph_from_adjacency_matrix(
#         wordbank_normal[rownames(wordbank_normal) %in% random_nodes,
#                colnames(wordbank_normal) %in% random_nodes],
#      weighted = TRUE, mode = "undirected")
# 
#       transitivity(random_graph)
# }
# 
# run_sim <- function(i) {
#   replicate(1000,sample_transitivity(i))
# }
# 
# # run from 2:653
# random_nets <- do.call("rbind", sapply(2:653, function(i) run_sim(i), simplify = FALSE))
# write.csv(random_nets, 'data/random_transitivity.csv')

#we will need to first drop the size column so it's not included
random_trans <- read_csv('data/random_transitivity.csv')  %>%
  select(-X1)

upper.random <- apply(random_trans, 1, function(x) quantile(x, probs=.975,na.rm = TRUE))
lower.random <- apply(random_trans, 1, function(x) quantile(x, probs=.025,na.rm = TRUE))
mean.random <- apply(random_trans, 1, function(x) mean(x, na.rm = TRUE))
random_trans <- data.frame(mean.random, upper.random, lower.random)
random_trans$num_words <- seq.int(nrow(random_nets))
random_trans$num_words <- (random_trans$num_words + 1)

compute_cis <- function(data_frame) {
    upper.random <- apply(data_frame, 1, function(x) quantile(x, probs=.975,na.rm = TRUE))
    lower.random <- apply(data_frame, 1, function(x) quantile(x, probs=.025,na.rm = TRUE))
    mean.random <- apply(data_frame, 1, function(x) mean(x, na.rm = TRUE))
    new_data <- data.frame(mean.random, upper.random, lower.random)
    new_data
}

#Add in a column specifying vocabulary size at a given iteration
# note that for some models, including this one, the vocab size is actually nrow+1
# because the model was starting with 2 nodes. 
new_data$num_words <- seq.int(nrow(random_nets))
random_trans$num_words <- (random_trans$num_words + 1)

#function to call for just one kid's data
pull_one <- function(id) {
  one_kid <- (english_data %>%
  filter(data_id == id) %>%
  filter(value == "produces"))
}

#function to make a graph of one kid's data and find CC
one_transitivity <- function(id) {
  one_graph <- graph_from_adjacency_matrix(
  wordbank_normal[rownames(wordbank_normal) %in% pull_one(id)$uni_lemma,
               colnames(wordbank_normal) %in% pull_one(id)$uni_lemma],
  weighted = TRUE, mode = "undirected")
  
  transitivity(one_graph)
}


#start a data frame
id <- 51699
transitivity <- one_transitivity(51699) 
transitivity_df <- data_frame(id, transitivity)

#loop in each kid's CC (17 minutes)
#slow, because for loop, using pull_one for comparison, and rbinding
# for (i in 51700:57474) {
#   if (nrow(pull_one(i))==1) {
#     transitivity_df <- rbind(transitivity_df, c(i, NA))
#   } else 
#     transitivity_df <- rbind(transitivity_df, c(i, one_transitivity(i)))
# }
# 
# write.csv(transitivity_df, 'individ_CC.csv')

transitivity_df <- read_csv('individ_CC.csv') %>%
  select(-X1)

transitivity_full <- left_join(transitivity_df, english_kids, by= c('id'='data_id'))


transitivity_production <- transitivity_full %>%
  filter(!is.na(production)) %>%
  group_by(production) %>%
  rename(num_words=production) %>%
  multi_boot_standard("transitivity", na.rm = TRUE) 

ggplot(aes(x = num_words, y = mean), data = transitivity_production) + 
  geom_pointrange(aes(ymax = ci_upper, ymin = ci_lower)) +
  geom_smooth() +
  theme_bw()






#looking at one kid
john_data <- eng_iteminfo_data %>%
  filter(data_id == 51699) %>%
  filter(value == "produces") 

john_matrix <- wordbank_normal[rownames(wordbank_normal) %in% john_data$uni_lemma,
               colnames(wordbank_normal) %in% john_data$uni_lemma] 
john_matrix[john_matrix <= .1] <- 0

john_graph <- graph_from_adjacency_matrix(john_matrix,
  weighted = TRUE, mode = "undirected")

transitivity(john_graph)



  
#longitudinal kids
repeated_administrations <- english_kids %>%
  group_by(original_id) %>%
  summarise(n = n())

longitudinal_kids <- english_data %>%
  left_join(english_kids) %>%
  filter(longitudinal) %>%
  distinct(original_id, age) %>%
  group_by(original_id) %>%
  summarise(n_ages = n())

transitivity_longitudinal <- transitivity_full %>%
  filter(longitudinal, form == "WS") %>%
  select(-id, -comprehension) %>%
  mutate(age = as.numeric(age)) %>%
  mutate(age = if_else(age == 17 | age == 29, age - 1, age)) %>%
  gather(measure, value, production, transitivity) %>%
  spread(age, value)

correlations_longitudinal <- transitivity_longitudinal %>%
  group_by(measure) %>%
  summarise(cor = cor(`16`, `28`, use = "complete"))


```


DEGREE: Random and Kid Data
```{r, echo=FALSE}
###  ###
#set a function for making a random graph of size N and computing degree (weighted)
sample_degree <- function(num_nodes) {
    random_nodes <- sample(all_items, num_nodes)

    random_graph <- graph_from_adjacency_matrix(
        wordbank_zeroed[rownames(wordbank_zeroed) %in% random_nodes,
               colnames(wordbank_zeroed) %in% random_nodes],
     weighted = TRUE, mode = "undirected")
    
    mean(graph.strength(random_graph))
}


run_sim_degree <- function(i) {
  replicate(1000,sample_degree(i))
}       
# random_nets_degree <- do.call("rbind", sapply(2:655, function(i) run_sim_degree(i), simplify = FALSE))


# write.csv(random_nets_degree, 'non_normalized/non_normalized_random_degree.csv')
# random_nets_degree <- read_csv('data/random_degree.csv')
random_nets_degree <- read_csv('non_normalized/non_normalized_random_degree.csv')

#we will need to first drop the size column so it's not included
random_nets_degree_dropped <- random_nets_degree %>%
  select(-X1)
  
upper.random <- apply(random_nets_degree_dropped, 1, 
                      function(x) quantile(x, probs=.975,na.rm = TRUE))
lower.random <- apply(random_nets_degree_dropped, 1, 
                      function(x) quantile(x, probs=.025,na.rm = TRUE))
mean.random <- apply(random_nets_degree_dropped, 1, 
                     function(x) mean(x, na.rm = TRUE))
random_degree <- data.frame(mean.random, upper.random, lower.random)
random_degree$num_words <- seq.int(nrow(random_nets_degree))
random_degree$num_words <- (random_degree$num_words + 1)


random_degree_avg <- random_degree %>%
  mutate(mean = mean.random/num_words,
         ci_upper = upper.random/num_words,
         ci_lower = lower.random/num_words)




one_degree <- function(id) {
  one_graph <- graph_from_adjacency_matrix(
  wordbank_zeroed[rownames(wordbank_zeroed) %in% pull_one(id)$uni_lemma,
               colnames(wordbank_zeroed) %in% pull_one(id)$uni_lemma],
  weighted = TRUE, mode = "undirected")
  
  mean(graph.strength(one_graph))
}

#start a data frame
id <- 51699
degree <- one_degree(51699)
degree_df <- data_frame(id, degree)
#loop in each kid's CC (17 minutes)
#slow, because for loop, using pull_one for comparison, and rbinding
#51700:57474
for (i in 51700:57474) {
  if (nrow(pull_one(i))==1) {
    degree_df <- rbind(degree_df, c(i, NA))
  } else
    degree_df <- rbind(degree_df, c(i, one_degree(i)))
}

# write.csv(degree_df, 'non_normalized/non_normal_kid_degree.csv')

# degree_df <- read_csv('data/individ_degree_strength.csv') %>%
#   select(-X1)

degree_df <- read_csv('non_normalized/non_normal_kid_degree.csv') %>%
  select(-X1)

degree_str_full <- left_join(degree_df, english_data, by= c('id'='data_id'))




degree_production <- degree_str_full %>%
  distinct(id, vocab_size, degree) %>%
  filter(!is.na(vocab_size)) %>%
  group_by(vocab_size) %>%
  multi_boot_standard("degree", na.rm = TRUE) 

degree_production_avg <- degree_production %>%
  mutate(mean = mean/vocab_size,
         ci_upper = ci_upper/vocab_size,
         ci_lower = ci_lower/vocab_size)

ggplot(aes(x = num_words, y = mean), data = random_degree_avg) +
  geom_ribbon(aes(ymax = ci_upper, ymin = ci_lower), fill='lightgrey') +
  geom_ribbon(aes(x = vocab_size, y = mean, ymax= ci_upper, ymin=ci_lower),
              data = degree_production_avg, fill='rosybrown') +
  geom_smooth(color='black') +
  geom_smooth(data=degree_production_avg, color= 'red', aes(x=vocab_size, y=mean)) +
  theme_bw()
```


GEODESICS: Random and Kid Data
```{r, echo=FALSE}
###  ###
#set a function for making a random graph of size N and computing geodesics
random_net <- function(num_nodes) {
    random_nodes <- sample(all_items, num_nodes)

    random_graph <- graph_from_adjacency_matrix(
        wordbank_normal[rownames(wordbank_normal) %in% random_nodes,
               colnames(wordbank_normal) %in% random_nodes],
     weighted = TRUE, mode = "undirected")
}

run_sim_geo <- function(i) {
  replicate(1000,average.path.length(random_net(i)))
}     

# ptm <- proc.time()
# random_nets_geo2 <- do.call("rbind", sapply(2:653, function(i) run_sim_geo(i), simplify = FALSE))
# proc.time() - ptm

# write.csv(random_nets_geo, 'random_geo.csv')
random_nets_geo <- read_csv('random_geo.csv') %>%
  select(-X1)
  
upper.random <- apply(random_nets_geo, 1, 
                      function(x) quantile(x, probs=.975,na.rm = TRUE))
lower.random <- apply(random_nets_geo, 1, 
                      function(x) quantile(x, probs=.025,na.rm = TRUE))
mean.random <- apply(random_nets_geo, 1, 
                     function(x) mean(x, na.rm = TRUE))
random_geo <- data.frame(mean.random, upper.random, lower.random)
random_geo$num_words <- seq.int(nrow(random_geo))
random_geo$num_words <- (random_geo$num_words + 1)



#### KID'S GEODESICS
one_geodesic <- function(id) {
  one_graph <- graph_from_adjacency_matrix(
  wordbank_normal[rownames(wordbank_normal) %in% pull_one(id)$uni_lemma,
               colnames(wordbank_normal) %in% pull_one(id)$uni_lemma],
  weighted = TRUE, mode = "undirected")
  
  average.path.length(one_graph)
}

#start a data frame
# id <- 51699
# geodesic <- one_geodesic(51699) 
# geodesic_df <- data_frame(id, geodesic)
# loop in each kid's CC (17 minutes)
# slow, because for loop, using pull_one for comparison, and rbinding
# 51700:57474
# for (i in 51700:57474) {
#   if (nrow(pull_one(i))==1) {
#     geodesic_df <- rbind(geodesic_df, c(i, NA))
#   } else 
#     geodesic_df <- rbind(geodesic_df, c(i, one_geodesic(i)))
# }
# write.csv(geodesic_df, 'individ_geodesic.csv')

geodesic_df <- read_csv('individ_geodesic.csv') %>%
  select(-X1)

geodesic_full <- left_join(geodesic_df, english_kids, by= c('id'='data_id'))



geodesic_production <- geodesic_full %>%
  filter(!is.na(production)) %>%
  #filter(production>1) %>%
  group_by(production) %>%
  multi_boot_standard("geodesic", na.rm = TRUE)

ggplot(aes(x = num_words, y = mean.random), data = random_geo) +
  geom_ribbon(aes(ymax = upper.random, ymin = lower.random), fill='lightgrey') +
  geom_ribbon(aes(x = production, y = mean, ymax= ci_upper, ymin=ci_lower),
              data = geodesic_production, fill='rosybrown') +
  geom_smooth(color='black') +
  geom_smooth(data=geodesic_production, aes(x=production, y=mean), color= 'red') +
  theme_bw()
```


ORDERED WORD NETWORKS based on the proportion of all kids that know each word
```{r, echo= FALSE}
kids_word_order <- english_data %>%
  group_by(uni_lemma) %>%
  filter(value=='produces') %>%
  filter(! is.na(uni_lemma)) %>%
  summarize(count= n()) %>%
  filter(uni_lemma %in% rownames(wordbank_zeroed)) %>%
  mutate(count = count/5775) %>%
  arrange(desc(count))

ordered_words <- kids_word_order$uni_lemma

build_net <- function(num_nodes) {
    random_nodes <- ordered_words[1:num_nodes]
    
    random_graph <- graph_from_adjacency_matrix(
        wordbank_normal[rownames(wordbank_normal) %in% random_nodes,
               colnames(wordbank_normal) %in% random_nodes],
     weighted = TRUE, mode = "undirected")
    random_graph
}


########## TRANSITIVITY ORDERED NETOWRKS ###################
# ordered_nets_trans <- do.call("rbind", sapply(2:653, function(i) 
#               transitivity(build_net(i)), simplify = FALSE))

# write.csv(ordered_nets_trans, 'ordered_nets_trans.csv')
ordered_nets_trans <- read_csv('ordered_nets_trans.csv') %>%
  rename(num_words=X1, transitivity=V1) %>%
  transmute(num_words=num_words+1, transitivity=transitivity)

ggplot(aes(x = num_words, y = transitivity), data = ordered_nets_trans) + 
  geom_ribbon(aes(x = num_words, y = mean, ymax= ci_upper, ymin=ci_lower),
              data = transitivity_production, fill='rosybrown') +
  geom_point(color='steelblue2') +
  geom_smooth() +
  geom_smooth(data = transitivity_production,aes(y=mean), color='red') +
  geom_smooth(data = random_trans, aes(y=mean.random), color= 'black') +
  theme_bw()




########### DEGREE ORDERED NETWORKS #############
# ordered_nets_degree <- do.call("rbind", sapply(2:653, function(i) 
#               mean(graph.strength(build_net(i))), simplify = FALSE))

# write.csv(ordered_nets_degree, 'ordered_nets_degree_str.csv')
ordered_nets_degree_avg <- read_csv('ordered_nets_degree_str.csv') %>%
  rename(num_words=X1, degree=V1) %>%
  transmute(num_words=num_words+1, degree=degree) %>%
  mutate(degree_avg = degree/num_words)

ggplot(aes(x = num_words, y = degree_avg), data = ordered_nets_degree_avg) + 
  geom_ribbon(aes(x = production, y = mean, ymax= ci_upper, ymin=ci_lower),
              data = degree_production_avg, fill='rosybrown') +
  geom_point(color='steelblue2') +
  geom_smooth() +
  geom_smooth(data = degree_production_avg,aes(x=production, y=mean), color='red') +
  geom_smooth(data = random_degree_avg, aes(y=mean), color= 'black') +
  theme_bw() +
  coord_cartesian(ylim = c(0, .05)) 




########### GEODESICS ORDERED NETWORKS #############
# ordered_nets_geo <- do.call("rbind", sapply(2:653, function(i) 
#               average.path.length(build_net(i)), simplify = FALSE))

# write.csv(ordered_nets_geo, 'ordered_nets_geo.csv')
ordered_nets_geo <- read_csv('ordered_nets_geo.csv') %>%
  rename(num_words=X1, geo=V1) %>%
  transmute(num_words=num_words+1, geo=geo)


ggplot(aes(x = num_words, y = geo), data = ordered_nets_geo) + 
  geom_ribbon(aes(x = production, y = mean, ymax= ci_upper, ymin=ci_lower),
              data = geodesic_production, fill='rosybrown') +
  geom_ribbon(aes(x= num_words, y = mean.random,
                  ymax = upper.random, ymin = lower.random), fill='skyblue') +
  geom_ribbon(aes(x = num_words, y = mean.random, ymax = upper.random, 
                  ymin = lower.random), data = random_geo, fill= 'lightgrey') +
  geom_point(color='steelblue2') +
  geom_smooth() +
  geom_smooth(data = geodesic_production,aes(x=production, y=mean), color='red') +
  geom_smooth(data = random_geo, aes(x=num_words, y=mean.random), color= 'black') +
  theme_bw() +
  coord_cartesian(ylim= c(1.1, 1.65))
```


Re-ordered: SWAPPING WORD ORDER TO EXAMINE EFFECTS ON MODEL
```{r, echo=FALSE}
ordered_words
ordered_words_swap <- kids_word_order
ordered_words_swap$rows <- seq.int(nrow(ordered_words_swap))

ordered_words_swap$rows <- ifelse(ordered_words_swap$rows<201, 
                          ordered_words_swap$rows + 700, ordered_words_swap$rows)
ordered_words_swap$rows <- ifelse(ordered_words_swap$rows>200 & 
                          ordered_words_swap$rows<401, 
                          ordered_words_swap$rows - 200, ordered_words_swap$rows)
ordered_words_swap$rows <- ifelse(ordered_words_swap$rows>700, 
                          ordered_words_swap$rows - 500, ordered_words_swap$rows)

reordered_words <- ordered_words_swap %>%
  arrange(rows)
reordered_words <- reordered_words$uni_lemma

build_net_reordered <- function(num_nodes) {
    random_nodes <- reordered_words[1:num_nodes]
    
    random_graph <- graph_from_adjacency_matrix(
        wordbank_normal[rownames(wordbank_normal) %in% random_nodes,
               colnames(wordbank_normal) %in% random_nodes],
     weighted = TRUE, mode = "undirected")
    random_graph
}


########## TRANSITIVITY RE-ORDERED NETOWRKS ###################
# reordered_trans <- do.call("rbind", sapply(2:653, function(i) 
#               transitivity(build_net_reordered(i)), simplify = FALSE))
# write.csv(reordered_trans, 'reordered_trans.csv')

reordered_trans <- read_csv('reordered_trans.csv') %>%
  rename(num_words=X1, transitivity=V1) %>%
  transmute(num_words=num_words+1, transitivity=transitivity)

ggplot(aes(x = num_words, y = mean), data = transitivity_production) + 
  geom_ribbon(data = random_trans, aes(x= num_words, y=mean.random, 
              ymax=upper.random, ymin=lower.random),fill= 'lightgrey') +
  geom_smooth(data = random_trans, aes(y=mean.random), color= 'black') +
  geom_ribbon(aes(ymax= ci_upper, ymin=ci_lower), fill='rosybrown') +
  geom_point(aes(x = num_words, y = transitivity),
             data = ordered_nets_trans, color = 'green') +
  geom_point(aes(x = num_words, y = transitivity), data = reordered_trans,
       color= 'steelblue2') +
  geom_smooth(data = reordered_trans,aes(y=transitivity), color='blue') +
  geom_smooth(color='red') +
  geom_smooth(aes(y = transitivity), data = ordered_nets_trans, color='forestgreen') +
  coord_cartesian(ylim= c(.6,.9)) +
  scale_x_continuous(breaks = seq(0, 650, 20)) +
  theme_bw()
```


PROBABILISTIC ordered networks 
```{r, echo= false}
build_net_prob <- function(num_nodes) {
    random_nodes <- kids_word_order %>%
      sample_n(num_nodes, weight = exp(100*count)) 
    
    random_nodes <- as.vector(random_nodes$uni_lemma)
    
    random_graph <- graph_from_adjacency_matrix(
        wordbank_normal[rownames(wordbank_normal) %in% random_nodes,
               colnames(wordbank_normal) %in% random_nodes],
     weighted = TRUE, mode = "undirected")
    random_graph
}


run_prob_sim <- function(i, FUN, iterations) {
  replicate(iterations, FUN(build_net_prob(i)))
}       

run_prob_sim(30, function(net) mean(graph.strength(net)), 10)


# Degree for a network built probabilistically
# prob_degree <- do.call("rbind", sapply(2:653, function(i) 
#   run_prob_sim(i, function(net) mean(graph.strength(net)), 100), simplify = FALSE))
prob_degree <- read_csv('prob_degree.csv') %>%
  select(-X1)
  
upper.random <- apply(prob_degree, 1, 
                      function(x) quantile(x, probs=.975,na.rm = TRUE))
lower.random <- apply(prob_degree, 1, 
                      function(x) quantile(x, probs=.025,na.rm = TRUE))
mean.random <- apply(prob_degree, 1, 
                     function(x) mean(x, na.rm = TRUE))
prob_degree <- data.frame(mean.random, upper.random, lower.random)
prob_degree$num_words <- seq.int(nrow(prob_degree))
prob_degree$num_words <- (prob_degree$num_words + 1)

prob_degree_avg <- prob_degree %>%
  mutate(mean = mean.random/num_words,
         ci_upper = upper.random/num_words,
         ci_lower = lower.random/num_words)

ggplot(aes(x = production, y = mean), data = degree_production_avg) + 
  geom_ribbon(data=random_degree_avg, aes(x=num_words, y=mean, ymax=ci_upper,
                                          ymin=ci_lower), color='lightgrey') +
  geom_ribbon(aes(ymax= ci_upper, ymin=ci_lower), fill='rosybrown') +
  geom_ribbon(aes(x=num_words, y=mean, ymax=ci_upper, ymin=ci_lower),
              data=prob_degree_avg, fill='steelblue') +
  geom_smooth(data=random_degree_avg, aes(x=num_words, y=mean), color='black') +
  geom_smooth(color='red') +
  #geom_smooth(aes(x=num_words, y=mean), data=prob_degree_avg, color='blue') +
  coord_cartesian(ylim=c(.01, .05)) +
  theme_bw()

ggplot(aes(x = production, y = mean), data = degree_production_avg) + 
  geom_pointrange(data=random_degree_avg, aes(x=num_words, y=mean, ymax=ci_upper,
                                          ymin=ci_lower), color='lightgrey') +
  geom_pointrange(aes(ymax= ci_upper, ymin=ci_lower), color='darkred', size = .2) +
  geom_pointrange(aes(x=num_words, y=mean, ymax=ci_upper, ymin=ci_lower),
              data=prob_degree_avg, color='steelblue', size = .2) +
  #geom_smooth(data=random_degree_avg, aes(x=num_words, y=mean), color='black') +
  #geom_smooth(color='red') +
  #geom_smooth(aes(x=num_words, y=mean), data=prob_degree_avg, color='blue') +
  coord_cartesian(ylim=c(.01, .05)) +
    scale_x_continuous(limits = c(10, 650)) +
  theme_bw()


#transitivity for probablistic networks
prob_transitivity <- do.call("rbind", sapply(2:653, function(i) 
  run_prob_sim(i, transitivity, 100), simplify = FALSE))


#geodesics for probablisitic networks
# prob_geodesics <- do.call("rbind", sapply(2:653, function(i) 
#   run_prob_sim(i, average.path.length, 100), simplify = FALSE))
prob_geo <- read_csv('prob_geodesics.csv') %>%
  select(-X1)
  
upper.random <- apply(prob_geo, 1, 
                      function(x) quantile(x, probs=.975,na.rm = TRUE))
lower.random <- apply(prob_geo, 1, 
                      function(x) quantile(x, probs=.025,na.rm = TRUE))
mean.random <- apply(prob_geo, 1, 
                     function(x) mean(x, na.rm = TRUE))
prob_geo <- data.frame(mean.random, upper.random, lower.random)
prob_geo$num_words <- seq.int(nrow(prob_geo))
prob_geo$num_words <- (prob_geo$num_words + 1)

ggplot(aes(x = production, y = mean), data = geodesic_production) + 
  geom_ribbon(data=random_geo, aes(x=num_words, y=mean.random, ymax=upper.random,
                                          ymin=lower.random), color='lightgrey') +
  geom_ribbon(aes(ymax= ci_upper, ymin=ci_lower), fill='rosybrown') +
  geom_ribbon(aes(x=num_words, y=mean.random, ymax=upper.random, ymin=lower.random),
              data=prob_geo, fill='steelblue') +
  geom_smooth(data=random_geo, aes(x=num_words, y=mean.random), color='black') +
  geom_smooth(color='red') +
  geom_smooth(aes(x=num_words, y=mean.random), data=prob_geo, color='blue') +
  #coord_cartesian(ylim=c(.01, .05)) +
  theme_bw()
```


PREFERENTIAL ACQUISITION MODELS
```{r, echo = FALSE}
# get the degree locally for each node in final network
wordbank_graph <- graph_from_adjacency_matrix(wordbank_normal,
                                    mode="undirected",weighted=TRUE)
final_node_degree <- graph.strength(wordbank_graph)
final_node_degree <- as.data.frame(final_node_degree)
final_node_degree$all_nodes <- rownames(final_node_degree) 

get_random_nodes <- function(num_nodes, df_to_join) {
    random_nodes <- sample(all_items, num_nodes)
    random_nodes <- as.data.frame(random_nodes)
    random_nodes <- left_join(random_nodes, df_to_join,
                        by= c('random_nodes' = 'all_nodes'))
    average_degree <- mean(random_nodes$final_node_degree)
    average_degree
}

get_prob_nodes <- function(num_nodes, df_to_join) {
    random_nodes <- kids_word_order %>%
      sample_n(num_nodes, weight = exp(100*count)) 
    random_nodes <- as.data.frame(random_nodes)
    random_nodes <- left_join(random_nodes, df_to_join,
                        by= c('uni_lemma' = 'all_nodes'))
    average_degree <- mean(random_nodes$final_node_degree)
    average_degree
}

run_node_sim <- function(i, df_to_join) {
  replicate(100, get_random_nodes(i, df_to_join))
}     

run_prob_node <- function(i, df_to_join) {
  replicate(100, get_prob_nodes(i, df_to_join))
}  
# acquisition_prob_degree <- do.call("rbind", sapply(2:653, 
#                    function(i) run_prob_node(i, final_node_degree), simplify = FALSE))

# acquisition_degree <- do.call("rbind", sapply(2:653, 
#                    function(i) run_node_sim(i, final_node_degree), simplify = FALSE))
# 
# write.csv(acquisition_degree, 'acquisition_degree.csv')
# write.csv(acquisition_prob_degree, 'acquisition_prob_degree.csv')

acquisition_degree <- read_csv('data/acquisition_degree.csv') %>%
  select(-X1)
  
upper.random <- apply(acquisition_degree, 1,
                      function(x) quantile(x, probs=.975,na.rm = TRUE))
lower.random <- apply(acquisition_degree, 1, 
                      function(x) quantile(x, probs=.025,na.rm = TRUE))
mean.random <- apply(acquisition_degree, 1, 
                     function(x) mean(x, na.rm = TRUE))
acquisition_degree <- data.frame(mean.random, upper.random, lower.random)
acquisition_degree$num_words <- seq.int(nrow(acquisition_degree))
acquisition_degree$num_words <- (acquisition_degree$num_words + 1)

acquisition_degree_avg <- acquisition_degree %>%
  mutate(mean = mean.random/655,
         ci_upper = upper.random/655,
         ci_lower = lower.random/655)



acquisition_prob_degree <- read_csv('acquisition_prob_degree.csv') %>%
  select(-X1)
  
upper.random <- apply(acquisition_prob_degree, 1, 
                      function(x) quantile(x, probs=.975,na.rm = TRUE))
lower.random <- apply(acquisition_prob_degree, 1, 
                      function(x) quantile(x, probs=.025,na.rm = TRUE))
mean.random <- apply(acquisition_prob_degree, 1, 
                     function(x) mean(x, na.rm = TRUE))
acquisition_prob_degree <- data.frame(mean.random, upper.random, lower.random)
acquisition_prob_degree$num_words <- seq.int(nrow(acquisition_prob_degree))
acquisition_prob_degree$num_words <- (acquisition_prob_degree$num_words + 1)

acquisition_prob_degree_avg <- acquisition_prob_degree %>%
  mutate(mean = mean.random/655,
         ci_upper = upper.random/655,
         ci_lower = lower.random/655)

ggplot(aes(x = num_words, y = mean), data = acquisition_degree_avg) + 
  geom_ribbon(aes(ymax= ci_upper, ymin=ci_lower), fill='rosybrown') +
  geom_ribbon(aes(x=num_words, y=mean, ymax=ci_upper, ymin=ci_lower),
              data=acquisition_prob_degree_avg, fill='steelblue') +
  geom_smooth(color='red') +
  geom_smooth(aes(x=num_words, y=mean), data=acquisition_prob_degree_avg, color='blue') +
  #coord_cartesian(ylim=c(.01, .05)) +
  theme_bw()




###### KIDS ACQUISITION MODELS!!! #######
kid_acquisition <- english_data %>%
  filter(value=='produces') %>%
  left_join(final_node_degree, by= c('uni_lemma'='all_nodes')) %>%
  group_by(data_id) %>%
  summarize(avg_degree = mean(final_node_degree)) %>%
  left_join(english_kids)

kid_acquisition_degree_production <- kid_acquisition %>%
  filter(!is.na(production)) %>%
  group_by(production) %>%
  rename(num_words=production) %>%
  mutate(avg_degree=avg_degree/654) %>%
  multi_boot_standard("avg_degree", na.rm = TRUE)

ggplot(aes(x = num_words, y = mean), data = kid_acquisition_degree_production) + 
  geom_pointrange(aes(x=num_words, y=mean, ymax=ci_upper, ymin=ci_lower),
              data=acquisition_degree_avg, color='lightgrey', size=.2) +
  geom_smooth(aes(x=num_words, y=mean), data=acquisition_degree_avg, color='black') +
  geom_pointrange(aes(ymax = ci_upper, ymin = ci_lower), color='rosybrown', size=.2) +
  geom_pointrange(aes(x=num_words, y=mean, ymax=ci_upper, ymin=ci_lower),
              data=acquisition_prob_degree_avg, color='steelblue', size=.2) +
  geom_smooth(aes(x=num_words, y=mean), data=acquisition_prob_degree_avg, color='blue') +
  geom_smooth(color='red') +
  theme_bw()
```


Look at correlation of Preferential Acquisition order and true order
```{r}
final_node_degree_sort <- final_node_degree %>% 
    arrange(desc(final_node_degree))
comparison_data <- left_join(kids_word_order, final_node_degree_sort,
                              by = c("uni_lemma" = "all_nodes"))

cor.test(comparison_data$count, scale(comparison_data$final_node_degree))

glm(count ~ scale(final_node_degree), family = "binomial", data = comparison_data)
```




UPPER BOUND: Making Networks Based on Maximizing Local Degree at Each Step
LURE OF ASSOCIATES?
```{r, echo=FALSE}
possible_start_nodes <- function(node1, node2) {
    possible_nodes <- c(all_items[node1], all_items[node2])

    if (node1!=node2) {
        possible_graph <- graph_from_adjacency_matrix(
            wordbank_normal[rownames(wordbank_normal) %in% possible_nodes,
                   colnames(wordbank_normal) %in% possible_nodes],
            weighted = TRUE, mode = "undirected")
        mean(graph.strength(possible_graph))
    } else 
        NA 
}


node_1 <- 11111
node_2 <- 11111
strength <- 11111
start_nodes <- data.frame(node_1, node_2, strength)

# very slow, also it doesn't need to calculate twice/pair
# for (i in 1:length(all_items)) {
#     for (y in 1:length(all_items)) {
#         start_nodes <- rbind(start_nodes, c(all_items[i], all_items[y],
#                                             possible_start_nodes(i,y))) 
#     }
# }

starting <- read_csv('data/start_nodes.csv') %>%
  arrange(desc(strength))


get_another_node <- function(vector_of_nodes, node2) {
    possible_nodes <- c(vector_of_nodes, node2)
    
    possible_graph <- graph_from_adjacency_matrix(
            wordbank_normal[rownames(wordbank_normal) %in% possible_nodes,
                   colnames(wordbank_normal) %in% possible_nodes],
            weighted = TRUE, mode = "undirected")
    mean(graph.strength(possible_graph))
}

# left_overs <- all_items
# maximized_nodes <- c('these', 'those')
# left_overs <- left_overs[-(which(left_overs %in% maximized_nodes))]
# 
# for (i in 1:653) {
#     more_nodes <- sapply(1:length(left_overs), function(i) 
#              get_another_node(maximized_nodes, left_overs[[i]]))
#     best_node <- left_overs[[which.max(more_nodes)]]
#     maximized_nodes <- c(maximized_nodes, best_node)
#     left_overs <- left_overs[-(which.max(more_nodes))]
# }

maximized_nodes <- read_csv('data/maximized_nodes.csv')
maximized_nodes <- as.vector(maximized_nodes$x)



build_net_max <- function(num_nodes) {
    random_nodes <- maximized_nodes[1:num_nodes]
    
    random_graph <- graph_from_adjacency_matrix(
        wordbank_normal[rownames(wordbank_normal) %in% random_nodes,
               colnames(wordbank_normal) %in% random_nodes],
     weighted = TRUE, mode = "undirected")
    random_graph
}

########## DEGREE LURE OF ASSOCIATES NETOWRKS ###################
maximized_nets <- do.call("rbind", sapply(2:655, function(i)
              mean(graph.strength(build_net_max(i))), simplify = FALSE))

maximized_nets <- read_csv('data/maximized_nets.csv') %>%
  rename(num_words=X1, degree=V1) %>%
  transmute(num_words=num_words+1, degree_avg= degree/num_words)

ggplot(aes(x=num_words, y=degree_avg), data=maximized_nets) +
  geom_point() +
  geom_smooth()

ggplot(aes(x = num_words, y = mean), data = random_degree_avg) +
  geom_ribbon(aes(ymax = ci_upper, ymin = ci_lower), fill='lightgrey') +
  geom_ribbon(aes(x = production, y = mean, ymax= ci_upper, ymin=ci_lower),
              data = degree_production_avg, fill='rosybrown') +
  geom_point(aes(x=num_words, y=degree_avg), data=maximized_nets, color='green') +
  geom_smooth(color='black') +
  geom_smooth(data=degree_production_avg, color= 'red', aes(x=production, y=mean)) +
  geom_smooth(data=maximized_nets, aes(x=num_words, y=degree_avg)) +
  #coord_cartesian(ylim=c(0,.14)) +
  theme_bw() 
```



True Preferential Attachment
```{r}
# we need a model where a new node is added if the nodes it would connect to have the highest average degree. 
# but wouldn't that privlege the case where a possible node only connects to one high degree node
#    over a possible node with that same connection, but that also connects to other nodes?
## Set/select starting nodes
attachment_nodes <- c('mommy')
# attachment_nodes <- as.vector(sixteen_month_words$uni_lemma)



#make a matrix that marks 0/1 all words as absent/present
build_current_net_matrix <- function(starting_nodes) {        
        node_nums <- (rownames(wordbank_zeroed) %in% starting_nodes)
        nodes <- ifelse(node_nums==TRUE, 1, 0)
        nodes <- as.matrix(nodes)
        rownames(nodes) <- all_items
        nodes
}

# This function returns a matrix with every node (both absent/present in current net) and its degree
#   but its degree only for considering the nodes currenlty in the network
# build_matrix_a <- function(starting_nodes) {
#         nodes <- build_current_net_matrix(starting_nodes)
#         t(nodes) %*% wordbank_zeroed
# }

# This function returns a matrix with every node in the net + its degree in the network at this step
build_matrix_c <- function(starting_nodes) {
        nodes <- build_current_net_matrix(starting_nodes)
        ((t(nodes) %*% wordbank_zeroed)) * t(nodes)
}

###SEQUENTIAL###########
#Recursive model that builds list of words, calculates global degree at each step
# can set which probability calculation to use.
get_matrix_nodes <- function(starting_nodes, num_words_to_add, initial_degree, model_type) {
        #Count (num_words_to_add) drops by 1 at each iteration. when all words have been generated,
        #  get output. 
        if (num_words_to_add == 0) {
              c(starting_nodes, initial_degree)
        #If the count is not up yet...
        } else {
              nodes <- build_current_net_matrix(starting_nodes)
              matrix_c <- build_matrix_c(starting_nodes)
              
              if (model_type == 'sequential') {
                        #special case where only one node is in net, auto select that node
                        chosen_node <- ifelse(sum(matrix_c)==0, which(nodes==1),
                                    sample(1:length(matrix_c), 1, prob = matrix_c))
                        chosen_matrix <- rep(0, length(matrix_c))
                        chosen_matrix[chosen_node] <- 1
                        #calcualte the probability of adding a given new node to the net
                        probability <- t(!nodes) * (t(chosen_matrix) %*% wordbank_zeroed)
                        #when adding the last few words it becomes increasingly likely that 
                        # for a given chosen word, there will be no words still to add that connect
                        # to it, which breaks the above code. 
                        #this while loop says to keep choosing nodes until that is no loger true. 
                        while (sum(probability)==0) {
                                chosen_node <- ifelse(sum(matrix_c)==0, which(nodes==1),
                                    sample(1:length(matrix_c), 1, prob = matrix_c))
                                chosen_matrix <- rep(0, length(matrix_c))
                                chosen_matrix[chosen_node] <- 1
                                probability <- t(!nodes) * (t(chosen_matrix) %*% wordbank_zeroed)
                        }
              } else if (model_type == 'simultaneous') {
                        #deal with special case where only one node is in network
                        if (sum(nodes==1)) {
                              probability <- t(!nodes) * (t(nodes) %*% wordbank_zeroed)
                        } else {
                              probability <- t(!nodes) * (matrix_c %*% wordbank_zeroed)
                        }
              } else if (model_type == 'loa') {
                        matrix_a <- t(nodes) %*% wordbank_zeroed
                        probability <- t(!nodes) * matrix_a 
              } else if (model_type == 'acquisition') {
                        probability <- t(!nodes) *  rowSums(wordbank_zeroed)
              }
              added_node <- sample(1:655, size=1, prob= probability)
              current_nodes <- c(starting_nodes, all_items[added_node])
              num_words_to_add <- num_words_to_add - 1
              tagged_nodes <- which(build_matrix_c(current_nodes) > 0)
              net_degree <- c(initial_degree, mean(build_matrix_c(current_nodes)[tagged_nodes]))
              get_matrix_nodes(current_nodes, num_words_to_add, net_degree, model_type)
        }
}

####### sequential ########
# 20000 iterations, takes about 16 hours
sequential_words_degree <- replicate(20000, 
        get_matrix_nodes(attachment_nodes, length(all_items) - length(attachment_nodes), 0, 
                      'sequential'))


####### SIMULTANEOUS ########
simultaneous_words_degree <- replicate(50, 
        get_matrix_nodes(attachment_nodes, length(all_items) - length(attachment_nodes), 0, 
                      'simultaneous'))

####### LURE OF THE ASSOCIATES ########
loa_words_degree <- replicate(20000, 
      get_matrix_nodes(attachment_nodes, length(all_items) - length(attachment_nodes), 0, 'loa'))


####### ACQUISITION ########
acquisition_words_degree <- replicate(1000, 
        get_matrix_nodes(attachment_nodes, length(all_items) - length(attachment_nodes), 0, 
                      'acquisition'))


# write.csv(sequential_words_degree, 'non_normalized/sequential_words_degree.csv')
# write.csv(loa_words_degree, 'non_normalized/loa_words_degree.csv')
# write.csv(acquisition_words_degree, 'non_normalized/acquisition_words_degree.csv')

sequential_words_degree <- read_csv('non_normalized/sequential_words_degree.csv') %>%
  select(-X1)
simultaneous_words_degree <- read_csv('non_normalized/simultaneous_words_degree.csv') %>%
  select(-X1)
loa_words_degree <- read_csv('non_normalized/loa_words_degree.csv') %>%
  select(-X1)
acquisition_words_degree <- read_csv('non_normalized/acquisition_words_degree.csv') %>%
  select(-X1)
  
compute_cis <- function(data_frame) {
    upper.random <- apply(data_frame, 1, function(x) quantile(x, probs=.975,na.rm = TRUE))
    lower.random <- apply(data_frame, 1, function(x) quantile(x, probs=.025,na.rm = TRUE))
    mean.random <- apply(data_frame, 1, function(x) mean(x, na.rm = TRUE))
    new_data <- data.frame(mean.random, upper.random, lower.random)
    new_data
}

clean_net_data <- function(data_frame) {
      #sequential_degree <- sequential_words_degree[1:1000]
      output_frame <- data_frame[-c(seq(1,by=1,len=length(all_items))),]
      
      # r thinks the variables are still strings, not numeric. fix.
      output_frame <- as.data.frame(sapply(output_frame, as.numeric))
      output_frame <- compute_cis(output_frame)
      output_frame$num_words <- seq.int(nrow(output_frame))
      output_frame_avg <- output_frame %>%
          transmute(mean = mean.random/num_words,
                 ci_upper = upper.random/num_words,
                 ci_lower = lower.random/num_words,
                 num_words=num_words)
      output_frame_avg
}

sequential_degree_avg <- clean_net_data(sequential_words_degree)
simultaneous_degree_avg <- clean_net_data(simultaneous_words_degree)
loa_degree_avg <- clean_net_data(loa_words_degree)
acquisition_degree_avg <- clean_net_data(acquisition_words_degree)

  
ggplot(data=random_degree_avg, x=num_words, y=mean) +
  geom_pointrange(aes(x=num_words, y=mean, ymax=ci_upper, ymin=ci_lower), color='lightgrey') +
  geom_pointrange(data=sequential_degree_avg, aes(x=num_words, y=mean, 
                  ymax=ci_upper, ymin=ci_lower), color='lightblue') +
  geom_pointrange(data=degree_production_avg, aes(x=vocab_size, y=mean, 
                  ymin=ci_lower, ymax=ci_upper), color='rosybrown') +
  geom_pointrange(data=acquisition_degree_avg, aes(x=num_words, y=mean, 
                 ymax=ci_upper, ymin=ci_lower), color='green') +
  coord_cartesian(xlim= c(0,20)) +
  theme_bw()



##### INVESTIGATING THE WORDS GENERATED BY MODELS#######
word_orders <- as.data.frame(matrix(NA, ncol = 1001, nrow = 1))
get_word_positions <- function(data_frame, how_many_words, initial_df_output)
    if(how_many_words==0) {
          as.data.frame(sapply(initial_df_output, as.numeric))
    } else {
          word_positions <- which(data_frame==all_items[how_many_words])
          word_positions_fixed <- 
              sapply(1:length(word_positions), function(i) (word_positions[i] - (655* (i-1))))
          new_row <- c(all_items[how_many_words], t(word_positions_fixed))
          word_orders_updated <- rbind(initial_df_output, new_row)
          this_many_more_words <- how_many_words - 1
          get_word_positions(data_frame, this_many_more_words, word_orders_updated)
    }

# function takes full data for each model (word+degrees), returns mean position + cis for each word
clean_word_positions <- function(data_frame) {
      #to get just the words, drop every row after all_items. 
      clean_df <- data_frame[(1:(nrow(data_frame)/2)),]
      clean_df <- get_word_positions(clean_df, length(all_items), word_orders) 
      clean_df <- clean_df[-1,]
      clean_df <- clean_df %>% select(-V1)
      clean_df_averages <- compute_cis(clean_df)
      clean_df_averages$this_word <- seq.int(nrow(clean_df_averages))
      clean_df_averages <- mutate(clean_df_averages, 
                                           this_word_uni = all_items[656- this_word])
      clean_df_averages$this_word_uni <- factor(clean_df_averages$this_word_uni, 
              levels = clean_df_averages$this_word_uni[order(clean_df_averages$mean.random)])
      clean_df_averages
}

acquisition_words_averages <- clean_word_positions(acquisition_words_degree)
loa_words_averages <- clean_word_positions(loa_words_degree)
simultaneous_words_averages <- clean_word_positions(simultaneous_words_degree)
sequential_words_averages <- clean_word_positions(sequential_words_degree)

sequential_words_categories <- left_join(sequential_words_averages, word_categories, 
                                       by=c('this_word_uni'= 'uni_lemma')) 
sequential_words_categories$this_word_uni <- factor(sequential_words_categories$this_word_uni, 
      levels = sequential_words_categories$this_word_uni[order(sequential_words_categories$mean.random)])


acquisition_words_categories <- left_join(acquisition_words_averages, word_categories, 
                                       by=c('this_word_uni'= 'uni_lemma')) 
acquisition_words_categories$this_word_uni <- factor(acquisition_words_categories$this_word_uni, 
      levels = acquisition_words_categories$this_word_uni[order(acquisition_words_categories$mean.random)])


ggplot(x=this_word_uni, y=mean.random, data=acquisition_words_categories) + 
  # geom_pointrange(aes(x=this_word_uni, y=mean.random,
                      # ymin=lower.random,ymax=upper.random), color='rosybrown') +
  geom_point(aes(x=this_word_uni, y=(mean.random), color='rosybrown')) +
  #coord_cartesian(xlim=c(1, 50)) +
  facet_wrap(~lexical_category)+
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
 

```



SCALE-FREE STATIC NETWORK?
```{r, echo=FALSE}
node_degrees <- data.frame(final_degree= rowSums(wordbank_zeroed))
# node_degrees <- data.frame(final_degree= rowSums(exp(4*wordbank_zeroed)-1))
node_degrees$uni_lemma <- rownames(node_degrees)
qplot(x=node_degrees$final_degree, binwidth=5) 

binned_degree_5 <- (.bincode(node_degrees$final_degree, breaks=seq.int(from = 0, 110, by= 5))) * 5
binned_node_degrees <- cbind(node_degrees, binned_degree_5) %>%
  group_by(binned_degree_5) %>%
  summarize(count=n()) %>%
  mutate(count_logged = log10(count),
         degree_logged= log10(binned_degree_5))
ggplot(data=binned_node_degrees) +
  geom_point(aes(x=degree_logged, y=count_logged))

binned_nodes_normal <- binned_node_degrees %>%
  mutate(count_normalized=(count_logged/(sum(count_logged))))
ggplot(data=binned_nodes_normal) +
  geom_point(aes(x=degree_logged, y=count_normalized))

degree_unweighted <- data.frame(final_degree= rowSums(wordbank_zeroed > 0))
degree_unweighted$uni_lemma <- rownames(node_degrees)
qplot(x=degree_unweighted$final_degree, binwidth=5) 






full_node_degrees <- data.frame(final_degree= rowSums(full_corpus_zeroed))
full_node_degrees$uni_lemma <- rownames(full_node_degrees)

word_categories <- english_data %>%
  distinct(uni_lemma, lexical_category)
full_degree_categories <- left_join(full_node_degrees, word_categories)

min_degrees <- seq(0, 1500, 50)

full_degree_dist <- data_frame(prop = sapply(min_degrees, function(x) mean(full_degree_categories$final_degree >= x)),
                               degree = min_degrees)

qplot(x = degree, y = prop, data = full_degree_dist) + 
  scale_x_log10() +  
  scale_y_log10() +
  theme_bw()


qplot(x = degree, y = prop, data = barabsi_degrees) + 
  scale_x_log10() +  
  scale_y_log10() +
  theme_bw()

qplot(x = 1:127, y = random_degrees) + 
  scale_x_log10() +  
  scale_y_log10() +
  theme_bw()

lexical_categories <- unique(full_degree_categories$lexical_category)

degree_dist <- sapply(lexical_categories, function(cat) {
  sapply(min_degrees, function(x) mean(filter(full_degree_categories, 
                                              lexical_category == cat)$final_degree >= x))
  }) %>%
  as.data.frame() %>%
  mutate(degree = min_degrees) %>%
  gather(lexical_category, prop, -degree)


qplot(x = degree, y = prop, data = degree_dist) + 
  facet_wrap(~lexical_category) +
  scale_x_log10() +  
  scale_y_log10() +
  theme_bw()




with(filter(wordbank_with_cat, lexical_category == "function_words"), cor(log10(word_count),final_degree))



#plot 2
word_categories <- english_data %>%
  distinct(uni_lemma, lexical_category)
degree_categories <- left_join(node_degrees, word_categories)
qplot(data=degree_categories, x=final_degree) +
  facet_wrap(~lexical_category) +
  scale_x_log10() +
  scale_y_log10()



degree_categories %>%
  group_by(lexical_category) %>%
  summarize(mean_degree = mean(final_degree),
            upper.degree = quantile(x=final_degree, probs=.975),
            lower.degree = quantile(x=final_degree, probs=.025))



### INCORPORATING FREQUENCY MEASURRES #####
wordbank_nouns <- word_categories %>% 
    filter(lexical_category=='nouns') 
childes_frequency <- read_csv('data/childes_english.csv') %>% 
  filter(word %in% rownames(wordbank_zeroed)) %>%
  filter(word %in% wordbank_nouns$uni_lemma)
#still need to deal with special cases. though also, more seem to be missing than just that.

wordbank_sim_nouns <- wordbank_zeroed[
                          rownames(wordbank_zeroed) %in% childes_frequency$word,
                          colnames(wordbank_zeroed) %in% childes_frequency$word]
wordbank_frequency$logged_count <- log10(childes_frequency$word_count)

frequency_matrix <- tcrossprod(childes_frequency$word_count)
wordbank_sim_frequency <- wordbank_sim_nouns * frequency_matrix

wordbank_degrees_with_freq <- data.frame(final_degree= rowSums(wordbank_zeroed))
wordbank_degrees_with_freq$uni_lemma <- rownames(wordbank_degrees_with_freq)
hist(wordbank_degrees_with_freq$final_degree)
hist(childes_degrees_with_freq$final_degree)

min_degrees <- seq(0, max(wordbank_degrees_with_freq$final_degree), 
                   max(wordbank_degrees_with_freq$final_degree)/50)

full_degree_dist <- data_frame(prop = sapply(min_degrees, function(x)
        mean(wordbank_degrees_with_freq$final_degree >= x)), degree = min_degrees)

qplot(x = degree, y = prop, data = full_degree_dist) + 
  scale_x_log10() +  
  scale_y_log10() +
  theme_bw()


### are the associations doing anything? ###
wordbank_identical <- ifelse(wordbank_zeroed > 0, .5, 0)




#discretizing
wordbank_zeroed_assocs <- read_csv('shiny_apps/networks/assocs/w2v_assocs.csv')

wordbank_long <- wordbank_zeroed_assocs %>%
 gather(target, assoc, -in_node)

wordbank_degrees <- wordbank_long %>%
 group_by(in_node) %>%
 summarise(degree = sum(assoc))

wordbank_degrees <- wordbank_long %>%
 group_by(in_node) %>%
 summarise(degree = sum(exp(9*assoc)))




###
english_data %>% 
    filter(value=='produces', age==18) %>% select(vocab_size, data_id)  %>% distinct(data_id, vocab_size)

john_words <- english_data %>% 
    filter(value=='produces') %>% 
    filter(data_id==51861) %>%
    select(uni_lemma)

john_degrees <- node_degrees %>% filter(uni_lemma %in% john_words$uni_lemma)

words_from_kids <- english_data %>% 
    filter(value=='produces', age==16) %>%
    select(uni_lemma)
words_from_kids_degrees <- left_join(words_from_kids, node_degrees)
hist(words_from_kids_degrees$final_degree)
```

```{r}
data_to_plot <- words_from_kids_degrees
min_degrees <- seq(0, max(data_to_plot$final_degree), 
                   max(data_to_plot$final_degree)/50)

full_degree_dist <- data_frame(prop = sapply(min_degrees, function(x)
        mean(data_to_plot$final_degree >= x)), degree = min_degrees)

qplot(x = degree, y = prop, data = full_degree_dist) + 
  scale_x_log10() +  
  scale_y_log10() +
  theme_bw()
```



BECKAGE ANALYSES: Erdos-Renyi
early vs late talkers?
```{r, echo=FALSE}
kids_degree <- degree_str_full %>%
  distinct(id, vocab_size, degree,age) %>%
  mutate(degree=(degree/vocab_size))
  
vocab_size_norms <- english_data %>% 
    distinct(data_id, age, vocab_size) %>%
    group_by(age) %>%
    summarize(average_vocab = median(vocab_size, na.rm=TRUE), sd_vocab=sd(vocab_size, na.rm=TRUE),
              low_vocab_10= quantile(vocab_size, probs=.10,na.rm = TRUE))

kids_degree_norms <- left_join(kids_degree, vocab_size_norms)



random_and_erdos_degree <- function(num_nodes) {
    random_nodes <- sample(all_items, num_nodes)

    random_matrix <- wordbank_zeroed[which(rownames(wordbank_zeroed) %in% random_nodes),
                                     which(colnames(wordbank_zeroed) %in% random_nodes)]
    
    
    erdos_shuffle <- sample(seq(from = 1, to = (num_nodes^2)))
    erdos_matrix <- matrix(random_matrix[erdos_shuffle], nrow=num_nodes, ncol=num_nodes)
    c(mean(rowSums(random_matrix)), mean(rowSums(erdos_matrix)))
}

run_sim_degree <- function(i) {
  replicate(1000,sample_degree(i))
}       
# random_nets_degree <- do.call("rbind", sapply(2:655, function(i) run_sim_degree(i), simplify = FALSE))



###### MARKOV CHAINS?
statesNames=c("a","b")
 mcA<-new("markovchain", transitionMatrix=matrix(c(0.7,0.3,0.1,0.9),byrow=TRUE,
 nrow=2, dimnames=list(statesNames,statesNames)))

statesNames=c("a","b","c")
mcB<-new("markovchain", states=statesNames, transitionMatrix=
          matrix(c(0.2,0.5,0.3,
                   0,1,0,
                   0.1,0.8,0.1),nrow=3, byrow=TRUE, dimnames=list(statesNames,
				   statesNames)
                 ))

statesNames=c("a","b","c","d")
matrice<-matrix(c(0.25,0.75,0,0,0.4,0.6,0,0,0,0,0.1,0.9,0,0,0.7,0.3), 
nrow=4, byrow=TRUE)
mcC<-new("markovchain", states=statesNames, transitionMatrix=matrice)
mcD<-new("markovchain", transitionMatrix=matrix(c(0,1,0,1), nrow=2,byrow=TRUE))

```





LONGITUDINAL kids
```{r, echo=FALSE}
repeated_administrations <- english_kids %>%
  group_by(original_id) %>%
  summarise(n = n())

longitudinal_kids <- english_data %>%
  left_join(english_kids) %>%
  filter(longitudinal) %>%
  distinct(original_id, age) %>%
  group_by(original_id) %>%
  summarise(n_ages = n())

kid_degree_longitudinal <- degree_str_full %>%
  filter(longitudinal, form == "WS") %>%
  select(-id) %>%
  mutate(age = as.numeric(age)) %>%
  mutate(age = if_else(age == 17 | age == 29, age - 1, age)) %>%
  gather(measure, value, vocab_size, degree) %>%
  spread(age, value)
```


Investigating Variability in Kid's Early Words
```{r, echo=false}
get_entropy <- function(num_words) {
  num_kids <- nrow( english_data %>%
    filter(vocab_size==num_words) %>%
    distinct(data_id))
    
  random_words <- replicate(num_kids, sample(all_items, num_words))
  
  some_vector <- sapply(1:length(all_items), 
                        function(i) sum(random_words %in% all_items[i]), simplify='vector')
  entropy.shrink(some_vector, verbose=FALSE)
}

#entropy function seems to normalize automatically
entropy(some_vector)
entropy(normalize(some_vector))
entropy(some_vector/655)

# ~9.5 hours
# entropy_dists <- sapply(1:655, function(i) replicate(1000, get_entropy(i)))
# write.csv(entropy_dists, 'data/vocab_entropy_dist.csv')
vocab_entropy <- read_csv('data/vocab_entropy_dist.csv') %>%
  select(-X1)

kids_vocab_entropy <- function(num_words) {
    kid_entropy <- english_data %>%
        filter(vocab_size==num_words) %>%
        filter(value=='produces') %>%
        group_by(uni_lemma) %>%
        summarize(num_kids=n())
    
    entropy.NSB(kid_entropy$num_kids, CMD='nsb-entropy')
}

kids_entropy <- sapply(1:5, function(i) kids_vocab_entropy(i))
kids_entropy_df <- data.frame(entropy = kids_entropy, vocab = 1:655)


get_sds <- function(vocab_size) {
  (mean(vocab_entropy[[vocab_size]]) - kids_vocab_entropy(vocab_size))/
    sd(vocab_entropy[[vocab_size]])
}

entropy_sds <- sapply(1:655, function(i) get_sds(i))
entropies <- as.data.frame(entropy_sds)
entropies$vocab_size <- seq.int(length(entropy_sds))

ggplot(aes(x= vocab_size, y=entropy_sds), data=entropies) +
  geom_point() +
  ylim(-20,175) +
  geom_smooth()
```



THINKING ABOUT THE HILLS STARTING NODES FOR OUR DATA
```{r, echo=FALSE}
#Hills starts with a network of words that >50% of kids know at 16 months
# But doesn't say exactly how many words that is for him...
# our kids definitely know some words 
english_data %>% 
    filter( value=='produces', age==16) %>% 
    group_by(vocab_size) %>% 
    summarize(n())

#953 kids at 16 months
english_data %>%
    filter(value=='produces') %>%
    distinct(data_id,age) %>%
    group_by(age) %>%
    summarize(num_kids = n())

#Find how many kids know each word for kids at 16 months
sixteen_month_words <- english_data %>%
    filter(value=='produces', age==16) %>%
    group_by(uni_lemma) %>%
    summarize(num_kids = n()/953) %>%
    arrange(desc(num_kids)) %>%
    filter(num_kids >.5)

#17 words that more than half of kids know at 16 months
#654 of 655 words are known by at least 1 child at 16 months
sixteen_month_words$uni_lemma <- factor(sixteen_month_words$uni_lemma, 
        levels = sixteen_month_words$uni_lemma[order(sixteen_month_words$num_kids)])
qplot(data=sixteen_month_words, x=uni_lemma, y=num_kids)
```





```{r, echo= FALSE}
#run the networks shiny app
runApp("shiny_apps/networks")
```


