---
title: "word2vec_morris"
output: html_document
---


```{r, echo=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(devtools))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(wordVectors))
suppressPackageStartupMessages(library(wordbankr))
suppressPackageStartupMessages(library(readr))
suppressPackageStartupMessages(library(shiny))
suppressPackageStartupMessages(library(shinyBS))
suppressPackageStartupMessages(library(visNetwork))
suppressPackageStartupMessages(library(langcog))
suppressPackageStartupMessages(library(tsne))
suppressPackageStartupMessages(library(networkD3))
suppressPackageStartupMessages(library(igraph))

#wordbank
# https://github.com/langcog/wordbank
eng_data <- get_instrument_data("English", "WS")

item_info <- read_csv("~/downloads/[English_WS].csv") %>%
 mutate(num_item_id = as.numeric(sub("item_", "", itemID))) %>%
 select(type, category, uni_lemma, num_item_id)

eng_iteminfo_data <- left_join(eng_data, item_info)



english_items <- get_item_data('English', 'WS') %>%
  filter(type =='word') %>%
  select(uni_lemma) %>%
  distinct() 
#NOTE: This drops "an" from the cdi. Might be other cases where two definitions map onto the same uni_lemma


# source: https://github.com/bmschmidt/wordVectors#quick-start

#"There's a lot of other stuff you can do besides just measuring nearness: you can do analogies, projection, and more complicated plots. But for that you should read my blog posts on this"
```




```{r, echo=FALSE, cache= TRUE}
#training the model on CHILDES
#full childes
# model = train_word2vec("CHILDES_words_only.txt",output="full_vectors.bin",
#             threads = 4,vectors = 100, window=6, cbow=1, min_count = 10, force= TRUE)

model = read.binary.vectors("full_vectors.bin")

#basic function to display the nearest elements for a specified target
nearest_to(model,model[["eat"]])
#special case words?
nearest_to(model,model[["choochoo"]])
nearest_to(model,model[["choo choo"]])
#same function expanded to multiple elements
nearest_to(model,model[[c("eat","finish","feed","spoil","cook","swallow","eats")]],20)
#use tnse package to plot in reduced dimensional space
some_eat = nearest_to(model,model[[c("eat","finish","feed","spoil","cook","swallow","eats")]],20)
plot(filter_to_rownames(model,names(some_eat)))

# Write a csv of the missing lemmas to join back in
# english_items_missing <- english_items %>%
  #filter(!uni_lemma %in% rownames(model)) %>%
  #write_csv("missing_lemmas.csv")

# Read back the csv with a new colum of replacement lemmas
missing_items <- read_csv("missing_lemmas.csv")

all_cdi_words <- unique(c(as.vector(as.matrix(missing_items)),
                        eng_iteminfo_data$uni_lemma))


#filter the matrix to only include CDI words
model_wordbank <-model[rownames(model) %in% all_cdi_words]
model_wordbank_sorted <-model_wordbank[sort(rownames(model_wordbank)),]

#Special cases
special_cases <- missing_items %>%
  filter(!is.na(addword) | !is.na(subword))

rename_cases <- missing_items %>%
  filter(is.na(addword) & is.na(subword),
         !is.na(word)) %>%
  arrange(uni_lemma)

extra_rows <- rownames(model_wordbank_sorted)  %in% rename_cases$word
rownames(model_wordbank_sorted)[extra_rows] <- rename_cases$uni_lemma

special_case_vector <- function(special_case) {
  if(!is.na(special_case$addword)) {
   model_wordbank[special_case$word,] + 
      model_wordbank[special_case$addword,]
  } else {
    model_wordbank[special_case$word,] - 
      model_wordbank[special_case$subword,]
  }
}

special_case_vectors <- t(sapply(1:nrow(special_cases), 
                               function(x) special_case_vector(special_cases[x,]), 
                               simplify = "matrix"))
rownames(special_case_vectors) <- special_cases$uni_lemma

model_wordbank_specialcases <- rbind(model_wordbank_sorted, special_case_vectors)

model_wordbank_unilemmas <- as.VectorSpaceModel(model_wordbank_specialcases[rownames(model_wordbank_specialcases)
                                                        %in% eng_iteminfo_data$uni_lemma, ])


#create word similarity matrix
wordbank_sim <- cosineSimilarity(model_wordbank_unilemmas, model_wordbank_unilemmas)

#set to zero all the negative values in the matrix
wordbank_zeroed <- ifelse(wordbank_sim < 0, 0, wordbank_sim)
#also set the self weights to zero before normalizing
diag(wordbank_zeroed) <- 0

#"normalize the rows of the matrix, such that each row sums to one, and that any value DM(I,J) can be interpreted as the strength of the directional connection from word WI to word WJ" (Rotaru, 2016)
wordbank_normal <- normalize_lengths(wordbank_zeroed)
hist(wordbank_normal)




#set a cutoff for edge connection, then make a graph
wordbank_connected <- ifelse(wordbank_sim < .1, 0, wordbank_sim)
wordbank_graph <- graph_from_adjacency_matrix(wordbank_connected,mode="undirected",weighted=TRUE)

distance_table(wordbank_graph, direct=FALSE)
mean_distance(wordbank_graph, direct=FALSE)
shortest_paths(wordbank_graph, from= "book", to= "milk", weights= NULL)

farthest_vertices(wordbank_graph)

# Rotaru 2016 also does the following step: "(f) consider the discrete Markov chain associated with DM, which we denote as MARKOV(DM), and compute the state of MARKOV(DM) at steps K = 1 through K = 7, namely SK(DM); (g) for each word W and each K between 1 and 7"

#write the matirx for visualization
#NOTE:if you rewrite the csv file, the network will NOT load when you run the shiny app
# until you manually enter 'in_node' (without quotes) as the value for the first cell of the data sheet.
#write.csv(wordbank_normal, file= 'shiny_apps/networks/assocs/w2v_assocs.csv')
```




```{r, echo=FALSE}
#Picture book training NOT USING THIS AT THE MOMENT
#NOTE: the picture book corpus needs cleaning, remove titles/authors
model = train_word2vec("100Books.txt",output="books_vectors.bin",threads = 3,vectors = 100,window=6, cbow=1, min_count = 10, force= TRUE)

#create word similarity matrix
books_sim <- cosineSimilarity(model,model)

#filter the matrix to only include CDI words
books_sim_wordbank <-books_sim[rownames(books_sim) %in% english_items$uni_lemma,
            colnames(books_sim) %in% english_items$uni_lemma]
books_scaled <- ((scale(books_sim_wordbank)))


english_items_missing <- english_items %>%
  filter(!uni_lemma %in% rownames(model)) %>%
  write_csv("book_missing_lemmas.csv")

missing_items <- read_csv("missing_lemmas.csv")

all_cdi_words <- unique(c(as.vector(as.matrix(missing_items)),
                        english_items$uni_lemma))

#filter the matrix to only include CDI words
model_wordbank <-model[rownames(model) %in% all_cdi_words]
model_wordbank_sorted <-model_wordbank[sort(rownames(model_wordbank)),]

#Special cases
special_cases <- missing_items %>%
  filter(!is.na(addword) | !is.na(subword))

rename_cases <- missing_items %>%
  filter(is.na(addword) & is.na(subword),
         !is.na(word)) %>%
  arrange(uni_lemma)

extra_rows <- rownames(model_wordbank_sorted)  %in% rename_cases$word
rownames(model_wordbank_sorted)[extra_rows] <- rename_cases$uni_lemma

special_case_vector <- function(special_case) {
  if(!is.na(special_case$addword)) {
   model_wordbank[special_case$word,] + 
      model_wordbank[special_case$addword,]
  } else {
    model_wordbank[special_case$word,] - 
      model_wordbank[special_case$subword,]
  }
}

special_case_vectors <- t(sapply(1:nrow(special_cases), 
                               function(x) special_case_vector(special_cases[x,]), 
                               simplify = "matrix"))
rownames(special_case_vectors) <- special_cases$uni_lemma

model_wordbank_specialcases <- rbind(model_wordbank_sorted, special_case_vectors)

model_wordbank_unilemmas <- as.VectorSpaceModel(model_wordbank_specialcases[rownames(model_wordbank_specialcases)
                                                        %in% english_items$uni_lemma, ])


#create word similarity matrix
wordbank_sim <- cosineSimilarity(model_wordbank_unilemmas, model_wordbank_unilemmas)

#set to zero all the negative values in the matrix
wordbank_zeroed <- ifelse(wordbank_sim < 0, 0, wordbank_sim)
#also set the self weights to zero before normalizing
diag(wordbank_zeroed) <- 0

#"normalize the rows of the matrix, such that each row sums to one, and that any value DM(I,J) can be interpreted as the strength of the directional connection from word WI to word WJ" (Rotaru, 2016)
wordbank_normal <- normalize_lengths(wordbank_zeroed)


#write.csv(books_sim_wordbank, 'books_word2vec.csv')

```




```{r}
english_data <- get_instrument_data("English", "WS", administrations = TRUE, 
                                    iteminfo=TRUE) %>%
  filter(type == "word", uni_lemma %in% rownames(wordbank_normal))



english_kids <- get_administration_data(language = "English")
                                        original_ids = TRUE) %>%
  distinct(original_id, age, longitudinal) %>%
  filter(longitudinal) 

all_items <- rownames(wordbank_normal)





#set a function for making a random graph of size N and computing CC
sample_transitivity <- function(num_nodes) {
    random_nodes <- sample(all_items, num_nodes)

    random_graph <- graph_from_adjacency_matrix(
        wordbank_normal[rownames(wordbank_normal) %in% random_nodes,
               colnames(wordbank_normal) %in% random_nodes],
     weighted = TRUE, mode = "undirected")

      transitivity(random_graph)
}

# sample_transitivity_threshold <- function(num_nodes) {
#     random_nodes <- sample(all_items, num_nodes)
# 
#     random_matrix <- wordbank_normal[rownames(wordbank_normal) %in% random_nodes,
#                colnames(wordbank_normal) %in% random_nodes]
#     random_matrix[random_matrix < .1] <- 0
#     
#     random_graph <- graph_from_adjacency_matrix(random_matrix,
#      weighted = TRUE, mode = "undirected")
# 
#       transitivity(random_graph)
# }

ptm <- proc.time()
run_sim <- function(i) {
  replicate(1000,sample_transitivity(i))
}       

# run_sim_threshold <- function(i) {
#   replicate(1000,sample_transitivity_threshold(i))
# }       


#run from 2:653
# random_nets <- do.call("rbind", sapply(2:653, function(i) run_sim(i), simplify = FALSE))
# proc.time() - ptm
# write.csv(random_nets, 'random_transitivity.csv')
random_nets <- read_csv('random_transitivity.csv')

#we will need to first drop the size column so it's not included
random_nets_dropped <- random_nets %>%
  select(-X1)
  
upper.random <- apply(random_nets_dropped, 1, 
                      function(x) quantile(x, probs=.975,na.rm = TRUE))
lower.random <- apply(random_nets_dropped, 1, 
                      function(x) quantile(x, probs=.025,na.rm = TRUE))
mean.random <- apply(random_nets_dropped, 1, function(x) mean(x, na.rm = TRUE))
random_trans <- data.frame(mean.random, upper.random, lower.random)
random_trans$num_words <- seq.int(nrow(random_nets))
random_trans$num_words <- (random_trans$num_words + 1)


ggplot(aes(x = num_words, y = mean.random), data = random_trans) + 
  geom_ribbon(aes(ymax = upper.random, ymin = lower.random)) +
  geom_ribbon(aes(x = production, y = mean, ymax= ci_upper, ymin=ci_lower),
              data = edited_trans_prod, color='red') +
  geom_smooth() +
  theme_bw()


transitivity_production[-c(1,2, 654:679),]
edited_trans_prod <- transitivity_production[-c(1,2, 654:679),]
  


#function to call for just one kid's data
pull_one <- function(id) {
  one_kid <- (eng_iteminfo_data %>%
  filter(data_id == id) %>%
  filter(value == "produces"))
}

#function to make a graph of one kid's data and find CC
one_transitivity <- function(id) {
  one_graph <- graph_from_adjacency_matrix(
  wordbank_normal[rownames(wordbank_normal) %in% pull_one(id)$uni_lemma,
               colnames(wordbank_normal) %in% pull_one(id)$uni_lemma],
  weighted = TRUE, mode = "undirected")
  
  transitivity(one_graph)
}

#problem kid has just one word --> breaks code because can't build net
one_transitivity(51795)
#if three words, outputs NaN
one_transitivity(51826)


#start a data frame
id <- 51699
transitivity <- one_transitivity(51699) 
transitivity_df <- data_frame(id, transitivity)

#loop in each kid's CC (17 minutes)
#slow, because for loop, using pull_one for comparison, and rbinding
for (i in 51700:57474) {
  if (nrow(pull_one(i))==1) {
    transitivity_df <- rbind(transitivity_df, c(i, NA))
  } else 
    transitivity_df <- rbind(transitivity_df, c(i, one_transitivity(i)))
}

write.csv(transitivity_df, 'individ_CC.csv')

transitivity_df <- read_csv('individ_CC.csv') %>%
  select(-X1)

transitivity_full <- left_join(transitivity_df, english_kids, by= c('id'='data_id'))

transitivity_longitudinal <- transitivity_full %>%
  filter(longitudinal, form == "WS") %>%
  select(-id, -comprehension) %>%
  mutate(age = as.numeric(age)) %>%
  mutate(age = if_else(age == 17 | age == 29, age - 1, age)) %>%
  gather(measure, value, production, transitivity) %>%
  spread(age, value)

correlations_longitudinal <- transitivity_longitudinal %>%
  group_by(measure) %>%
  summarise(cor = cor(`16`, `28`, use = "complete"))



ggplot(aes(x = age, y = transitivity), data = transitivity_full) + 
  geom_jitter() +
  theme_bw()

transitivity_age <- transitivity_full %>%
  filter(!is.na(age)) %>%
  group_by(age) %>%
  multi_boot_standard("transitivity", na.rm = TRUE) 

ggplot(aes(x = age, y = mean), data = transitivity_age) + 
  geom_pointrange(aes(ymax = ci_upper, ymin = ci_lower)) +
  geom_smooth() +
  theme_bw()

transitivity_production <- transitivity_full %>%
  filter(!is.na(production)) %>%
  group_by(production) %>%
  multi_boot_standard("transitivity", na.rm = TRUE) 

ggplot(aes(x = production, y = mean), data = transitivity_production) + 
  geom_pointrange(aes(ymax = ci_upper, ymin = ci_lower)) +
  geom_smooth() +
  theme_bw()


english_kids <- get_administration_data(language = "English", form = "WS", original_ids = TRUE)




john_data <- eng_iteminfo_data %>%
  filter(data_id == 51699) %>%
  filter(value == "produces") 

john_matrix <- wordbank_normal[rownames(wordbank_normal) %in% john_data$uni_lemma,
               colnames(wordbank_normal) %in% john_data$uni_lemma] 
john_matrix[john_matrix <= .1] <- 0

john_graph <- graph_from_adjacency_matrix(john_matrix,
  weighted = TRUE, mode = "undirected")

transitivity(john_graph)



  
#longitudinal kids
repeated_administrations <- english_kids %>%
  group_by(original_id) %>%
  summarise(n = n())

longitudinal_kids <- english_data %>%
  left_join(english_kids) %>%
  filter(longitudinal) %>%
  distinct(original_id, age) %>%
  group_by(original_id) %>%
  summarise(n_ages = n())




```





```{r, echo=FALSE}
### DEGREE ###
#set a function for making a random graph of size N and computing degree (weighted)
sample_degree <- function(num_nodes) {
    random_nodes <- sample(all_items, num_nodes)

    random_graph <- graph_from_adjacency_matrix(
        wordbank_normal[rownames(wordbank_normal) %in% random_nodes,
               colnames(wordbank_normal) %in% random_nodes],
     weighted = TRUE, mode = "undirected")
    
    mean(graph.strength(random_graph))
}


ptm <- proc.time()
run_sim_degree <- function(i) {
  replicate(1000,sample_degree(i))
}       

#2:653
random_nets_degree <- do.call("rbind", sapply(2:653, function(i) run_sim_degree(i), simplify = FALSE))
proc.time() - ptm





one_degree <- function(id) {
  one_graph <- graph_from_adjacency_matrix(
  wordbank_normal[rownames(wordbank_normal) %in% pull_one(id)$uni_lemma,
               colnames(wordbank_normal) %in% pull_one(id)$uni_lemma],
  weighted = TRUE, mode = "undirected")
  
  mean(graph.strength(one_graph))
}

#problem kid has just one word --> breaks code because can't build net
one_degree(51795)
#if three words, outputs a value
one_degree(51699)

#start a data frame
id <- 51699
degree <- one_degree(51699) 
degree_df <- data_frame(id, degree)

#loop in each kid's CC (17 minutes)
#slow, because for loop, using pull_one for comparison, and rbinding
# 51700:57474
for (i in 51700:57474) {
  if (nrow(pull_one(i))==1) {
    degree_df <- rbind(degree_df, c(i, NA))
  } else 
    degree_df <- rbind(degree_df, c(i, one_degree(i)))
}

write.csv(degree_df, 'individ_degree_strength.csv')

degree_df <- read_csv('individ_degree_strength.csv') %>%
  select(-X1)

#degree_full <- left_join(degree_df, english_kids, by= c('id'='data_id'))
degree_str_full <- left_join(degree_df, english_kids, by= c('id'='data_id'))

hist(degree_str_full$degree)

degree_longitudinal <- degree_str_full %>%
  filter(longitudinal, form == "WS") %>%
  select(-id, -comprehension) %>%
  mutate(age = as.numeric(age)) %>%
  mutate(age = if_else(age == 17 | age == 29, age - 1, age)) %>%
  gather(measure, value, production, degree) %>%
  spread(age, value)

correlations_longitudinal <- degree_longitudinal %>%
  group_by(measure) %>%
  summarise(cor = cor(`16`, `28`, use = "complete"))
correlations_longitudinal

ggplot(aes(x = age, y = degree), data = degree_str_full) + 
  geom_jitter() +
  theme_bw()

ggplot(aes(x = production, y = degree), data = degree_str_full) + 
  geom_jitter() +
  theme_bw()

degree_age <- degree_str_full %>%
  filter(!is.na(age)) %>%
  group_by(age) %>%
  multi_boot_standard("degree", na.rm = TRUE) 

ggplot(aes(x = age, y = mean), data = degree_age) + 
  geom_pointrange(aes(ymax = ci_upper, ymin = ci_lower)) +
  geom_smooth() +
  theme_bw()

degree_production <- degree_full %>%
  filter(!is.na(production)) %>%
  group_by(production) %>%
  multi_boot_standard("degree", na.rm = TRUE) 

degree_production_avg <- degree_production %>%
  mutate(mean = mean/production,
         ci_upper = ci_upper/production,
         ci_lower = ci_lower/production)

ggplot(aes(x = production, y = mean), data = degree_production_avg) + 
  geom_pointrange(aes(ymax = ci_upper, ymin = ci_lower)) +
  geom_smooth() +
  theme_bw()
```




```{r, echo=FALSE}
### GEODESICS ###
one_geodesic <- function(id) {
  one_graph <- graph_from_adjacency_matrix(
  wordbank_normal[rownames(wordbank_normal) %in% pull_one(id)$uni_lemma,
               colnames(wordbank_normal) %in% pull_one(id)$uni_lemma],
  weighted = TRUE, mode = "undirected")
  
  average.path.length(one_graph)
}

#problem kid has just one word --> breaks code because can't build net
one_geodesic(51795)
#if three words, outputs a value
one_geodesic(51699)

#start a data frame
id <- 51699
geodesic <- one_geodesic(51699) 
geodesic_df <- data_frame(id, geodesic)

#loop in each kid's CC (17 minutes)
#slow, because for loop, using pull_one for comparison, and rbinding
# 51700:57474
for (i in 51700:57474) {
  if (nrow(pull_one(i))==1) {
    geodesic_df <- rbind(geodesic_df, c(i, NA))
  } else 
    geodesic_df <- rbind(geodesic_df, c(i, one_geodesic(i)))
}

write.csv(geodesic_df, 'individ_geodesic.csv')

geodesic_df <- read_csv('individ_geodesic.csv') %>%
  select(-X1)

geodesic_full <- left_join(geodesic_df, english_kids, by= c('id'='data_id'))

hist(geodesic_full$geodesic)

geodesic_longitudinal <- geodesic_full %>%
  filter(longitudinal, form == "WS") %>%
  select(-id, -comprehension) %>%
  mutate(age = as.numeric(age)) %>%
  mutate(age = if_else(age == 17 | age == 29, age - 1, age)) %>%
  gather(measure, value, production, geodesic) %>%
  spread(age, value)

correlations_longitudinal <- geodesic_longitudinal %>%
  group_by(measure) %>%
  summarise(cor = cor(`16`, `28`, use = "complete"))
correlations_longitudinal

ggplot(aes(x = age, y = geodesic), data = geodesic_full) + 
  geom_jitter() +
  theme_bw()

ggplot(aes(x = production, y = geodesic), data = geodesic_full) + 
  geom_jitter() +
  theme_bw()

geodesic_age <- geodesic_full %>%
  filter(!is.na(age)) %>%
  group_by(age) %>%
  multi_boot_standard("geodesic", na.rm = TRUE) 

ggplot(aes(x = age, y = mean), data = geodesic_age) + 
  geom_pointrange(aes(ymax = ci_upper, ymin = ci_lower)) +
  geom_smooth() +
  theme_bw()

geodesic_production <- geodesic_full %>%
  filter(!is.na(production)) %>%
  filter(production>10) %>%
  group_by(production) %>%
  multi_boot_standard("geodesic", na.rm = TRUE) 

ggplot(aes(x = production, y = mean), data = geodesic_production) + 
  geom_pointrange(aes(ymax = ci_upper, ymin = ci_lower)) +
  geom_smooth() +
  theme_bw()
```



```{r, echo= FALSE}
#run the networks shiny app
runApp("shiny_apps/networks")
```


